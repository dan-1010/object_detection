{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7120ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import pyttsx3\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04850f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8 model (medium size; change to 'yolov8n.pt' for faster inference)\n",
    "model = YOLO('yolov8m.pt')\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "tts = pyttsx3.init()\n",
    "tts.setProperty('rate', 150)  # Optional: slower speech for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98fce7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to ESP32-CAM stream\n"
     ]
    }
   ],
   "source": [
    "# Replace with your ESP32-CAM stream URL\n",
    "stream_url = \"http://192.168.1.20:81/stream\"\n",
    "cap = cv2.VideoCapture(stream_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Failed to connect to ESP32-CAM stream\")\n",
    "else:\n",
    "    print(\"✅ Connected to ESP32-CAM stream\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f958bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only warn about these objects\n",
    "DANGEROUS_CLASSES = {\n",
    "    \"knife\", \"scissors\", \"truck\", \"bus\", \"car\", \"motorbike\",\n",
    "    \"dog\", \"bear\", \"bottle\", \"fire hydrant\"\n",
    "}\n",
    "\n",
    "# Keep track of recently announced objects\n",
    "recent_detections = set()\n",
    "last_reset_time = time.time()\n",
    "\n",
    "def speak(text):\n",
    "    tts.say(text)\n",
    "    tts.runAndWait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc8c66f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cap.isOpened():\n",
    "#     print(\"🎥 Press 'q' to stop\")\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             print(\"⚠️ Failed to grab frame\")\n",
    "#             break\n",
    "\n",
    "#         # Run YOLO detection\n",
    "#         results = model(frame, verbose=False)\n",
    "\n",
    "#         # Annotate frame\n",
    "#         annotated = results[0].plot()\n",
    "\n",
    "#         # Display frame\n",
    "#         cv2.imshow(\"ESP32-CAM + YOLOv8\", annotated)\n",
    "\n",
    "#         # Handle detections\n",
    "#         for box in results[0].boxes:\n",
    "#             cls_id = int(box.cls[0])\n",
    "#             conf = float(box.conf[0])\n",
    "#             name = model.names[cls_id]\n",
    "\n",
    "#             if name in DANGEROUS_CLASSES and name not in recent_detections:\n",
    "#                 print(f\"⚠️ {name} detected ({conf:.2f})\")\n",
    "#                 speak(f\"Warning: {name} ahead\")\n",
    "#                 recent_detections.add(name)\n",
    "\n",
    "#         # Clear memory every 5 seconds\n",
    "#         if time.time() - last_reset_time > 5:\n",
    "#             recent_detections.clear()\n",
    "#             last_reset_time = time.time()\n",
    "\n",
    "#         # Exit condition\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "# # Cleanup\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525ecc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32f4f424",
   "metadata": {},
   "source": [
    "# Simple text to speech object detetcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ee6cd0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m⚠️ Failed to get frame\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m annotated_frame = results[\u001b[32m0\u001b[39m].plot()\n\u001b[32m     18\u001b[39m cv2.imshow(\u001b[33m\"\u001b[39m\u001b[33mLive Detection\u001b[39m\u001b[33m\"\u001b[39m, annotated_frame)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\Desktop\\Projects\\recom_env\\Lib\\site-packages\\ultralytics\\engine\\model.py:185\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, source, stream, **kwargs)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    157\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    158\u001b[39m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image.Image, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np.ndarray, torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    159\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    160\u001b[39m     **kwargs: Any,\n\u001b[32m    161\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[32m    164\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m \u001b[33;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\Desktop\\Projects\\recom_env\\Lib\\site-packages\\ultralytics\\engine\\model.py:548\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor:\n\u001b[32m    547\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor = (predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._smart_load(\u001b[33m\"\u001b[39m\u001b[33mpredictor\u001b[39m\u001b[33m\"\u001b[39m))(overrides=args, _callbacks=\u001b[38;5;28mself\u001b[39m.callbacks)\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_cli\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n\u001b[32m    550\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.args = get_cfg(\u001b[38;5;28mself\u001b[39m.predictor.args, args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\Desktop\\Projects\\recom_env\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:385\u001b[39m, in \u001b[36mBasePredictor.setup_model\u001b[39m\u001b[34m(self, model, verbose)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msetup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, verbose: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    378\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[33;03m    Initialize YOLO model with given parameters and set it to evaluation mode.\u001b[39;00m\n\u001b[32m    380\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m \u001b[33;03m        verbose (bool): Whether to print verbose output.\u001b[39;00m\n\u001b[32m    384\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mAutoBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdnn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfuse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    396\u001b[39m     \u001b[38;5;28mself\u001b[39m.device = \u001b[38;5;28mself\u001b[39m.model.device  \u001b[38;5;66;03m# update device\u001b[39;00m\n\u001b[32m    397\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.half = \u001b[38;5;28mself\u001b[39m.model.fp16  \u001b[38;5;66;03m# update half\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\Desktop\\Projects\\recom_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\Desktop\\Projects\\recom_env\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:200\u001b[39m, in \u001b[36mAutoBackend.__init__\u001b[39m\u001b[34m(self, weights, device, dnn, data, fp16, batch, fuse, verbose)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nn_module:\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m fuse:\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m         weights = \u001b[43mweights\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# fuse before move to gpu\u001b[39;00m\n\u001b[32m    201\u001b[39m     model = weights.to(device)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mkpt_shape\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\Desktop\\Projects\\recom_env\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:236\u001b[39m, in \u001b[36mBaseModel.fuse\u001b[39m\u001b[34m(self, verbose)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, Conv2):\n\u001b[32m    235\u001b[39m     m.fuse_convs()\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m m.conv = \u001b[43mfuse_conv_and_bn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbn\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# update conv\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[38;5;28mdelattr\u001b[39m(m, \u001b[33m\"\u001b[39m\u001b[33mbn\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# remove batchnorm\u001b[39;00m\n\u001b[32m    238\u001b[39m m.forward = m.forward_fuse  \u001b[38;5;66;03m# update forward\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\Desktop\\Projects\\recom_env\\Lib\\site-packages\\ultralytics\\utils\\torch_utils.py:271\u001b[39m, in \u001b[36mfuse_conv_and_bn\u001b[39m\u001b[34m(conv, bn)\u001b[39m\n\u001b[32m    269\u001b[39m w_conv = conv.weight.view(conv.out_channels, -\u001b[32m1\u001b[39m)\n\u001b[32m    270\u001b[39m w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m fusedconv.weight.copy_(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_bn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_conv\u001b[49m\u001b[43m)\u001b[49m.view(fusedconv.weight.shape))\n\u001b[32m    273\u001b[39m \u001b[38;5;66;03m# Prepare spatial bias\u001b[39;00m\n\u001b[32m    274\u001b[39m b_conv = (\n\u001b[32m    275\u001b[39m     torch.zeros(conv.weight.shape[\u001b[32m0\u001b[39m], dtype=conv.weight.dtype, device=conv.weight.device)\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m conv.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m conv.bias\n\u001b[32m    278\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# For avoiding repeat announcements\n",
    "recent_objects = set()\n",
    "last_reset = time.time()\n",
    "\n",
    "def speak(text):\n",
    "    tts.say(text)\n",
    "    tts.runAndWait()\n",
    "\n",
    "if cap.isOpened():\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"⚠️ Failed to get frame\")\n",
    "            break\n",
    "\n",
    "        results = model(frame, verbose=False)\n",
    "        annotated_frame = results[0].plot()\n",
    "        cv2.imshow(\"Live Detection\", annotated_frame)\n",
    "\n",
    "        # Loop through detected objects\n",
    "        for box in results[0].boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            class_name = model.names[cls_id]\n",
    "\n",
    "            if class_name not in recent_objects:\n",
    "                print(f\"🗣️ Saying: {class_name}\")\n",
    "                speak(class_name)\n",
    "                recent_objects.add(class_name)\n",
    "\n",
    "        # Reset memory every 5 seconds to allow re-speaking\n",
    "        if time.time() - last_reset > 5:\n",
    "            recent_objects.clear()\n",
    "            last_reset = time.time()\n",
    "\n",
    "        # Quit when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release camera and window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc1be6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b035fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186590f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING torchvision==0.20 is incompatible with torch==2.6.\n",
      "Run 'pip install torchvision==0.21' to fix torchvision or 'pip install -U torch torchvision' to update both.\n",
      "For a full compatibility table see https://github.com/pytorch/vision#installation\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import pyttsx3\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8 model (medium size)\n",
    "model = YOLO('yolov8m.pt')\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "tts = pyttsx3.init()\n",
    "tts.setProperty('rate', 150)  # Adjust speech speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1065b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to ESP32-CAM stream\n"
     ]
    }
   ],
   "source": [
    "# ESP32-CAM stream URL\n",
    "stream_url = \"http://192.168.1.20:81/stream\"\n",
    "cap = cv2.VideoCapture(stream_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Failed to connect to ESP32-CAM stream\")\n",
    "else:\n",
    "    print(\"✅ Connected to ESP32-CAM stream\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24123afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 Press 'q' to stop\n",
      "🗣️ Saying: person nearby. (Confidence: 0.31)\n",
      "🗣️ Saying: person nearby. (Confidence: 0.44)\n",
      "🗣️ Saying: tie (Confidence: 0.29)\n",
      "🗣️ Saying: person nearby. (Confidence: 0.48)\n",
      "🗣️ Saying: person nearby. (Confidence: 0.26)\n",
      "🗣️ Saying: cat nearby. (Confidence: 0.43)\n",
      "🗣️ Saying: cat nearby. (Confidence: 0.29)\n",
      "🗣️ Saying: cat nearby. (Confidence: 0.25)\n",
      "🗣️ Saying: person nearby. (Confidence: 0.26)\n",
      "🗣️ Saying: person nearby. (Confidence: 0.46)\n",
      "🗣️ Saying: Obstacle ahead: laptop. (Confidence: 0.27)\n",
      "🗣️ Saying: cat nearby. (Confidence: 0.27)\n",
      "🗣️ Saying: tie (Confidence: 0.26)\n",
      "🗣️ Saying: cat nearby. (Confidence: 0.30)\n",
      "🗣️ Saying: Obstacle ahead: remote. (Confidence: 0.30)\n",
      "🗣️ Saying: tie (Confidence: 0.32)\n",
      "🗣️ Saying: person nearby. (Confidence: 0.26)\n",
      "🗣️ Saying: tie (Confidence: 0.38)\n",
      "🗣️ Saying: cat nearby. (Confidence: 0.28)\n",
      "🗣️ Saying: tie (Confidence: 0.40)\n",
      "🗣️ Saying: Warning: dog detected. Be careful. (Confidence: 0.28)\n",
      "🗣️ Saying: tie (Confidence: 0.60)\n",
      "🗣️ Saying: Obstacle ahead: remote. (Confidence: 0.33)\n",
      "🗣️ Saying: cat nearby. (Confidence: 0.25)\n",
      "🗣️ Saying: cat nearby. (Confidence: 0.45)\n",
      "🗣️ Saying: tie (Confidence: 0.31)\n",
      "🗣️ Saying: cat nearby. (Confidence: 0.33)\n",
      "🗣️ Saying: cat nearby. (Confidence: 0.40)\n",
      "🗣️ Saying: tie (Confidence: 0.27)\n",
      "🗣️ Saying: tie (Confidence: 0.31)\n",
      "🗣️ Saying: cat nearby. (Confidence: 0.77)\n",
      "🗣️ Saying: cat nearby. (Confidence: 0.32)\n",
      "🗣️ Saying: Obstacle ahead: laptop. (Confidence: 0.76)\n",
      "🗣️ Saying: Obstacle ahead: laptop. (Confidence: 0.55)\n",
      "🗣️ Saying: cat nearby. (Confidence: 0.35)\n",
      "🗣️ Saying: tie (Confidence: 0.25)\n",
      "🗣️ Saying: person nearby. (Confidence: 0.27)\n",
      "🗣️ Saying: tie (Confidence: 0.27)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Object Categories\n",
    "dangerous_objects = {\n",
    "    \"knife\", \"scissors\", \"fire hydrant\", \"train\", \"bus\", \"truck\", \"car\", \"motorcycle\",\n",
    "    \"bicycle\", \"airplane\", \"boat\", \"skateboard\", \"surfboard\", \"baseball bat\",\n",
    "    \"baseball glove\", \"tennis racket\", \"microwave\", \"oven\", \"toaster\", \"sink\",\n",
    "    \"refrigerator\", \"hair drier\", \"toilet\", \"bear\", \"dog\"\n",
    "}\n",
    "\n",
    "obstacle_objects = {\n",
    "    \"bench\", \"chair\", \"couch\", \"dining table\", \"bed\", \"tv\", \"vase\", \"potted plant\",\n",
    "    \"parking meter\", \"stop sign\", \"traffic light\", \"remote\", \"keyboard\", \"laptop\",\n",
    "    \"mouse\", \"cell phone\", \"book\", \"clock\", \"suitcase\", \"backpack\", \"handbag\", \"umbrella\"\n",
    "}\n",
    "\n",
    "living_objects = {\n",
    "    \"person\", \"dog\", \"cat\", \"bird\", \"horse\", \"sheep\", \"cow\", \"elephant\",\n",
    "    \"bear\", \"zebra\", \"giraffe\"\n",
    "}\n",
    "\n",
    "# Track recent announcements\n",
    "recent_detections = set()\n",
    "last_reset_time = time.time()\n",
    "\n",
    "def speak(text):\n",
    "    tts.say(text)\n",
    "    tts.runAndWait()\n",
    "\n",
    "if cap.isOpened():\n",
    "    print(\"🎥 Press 'q' to stop\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"⚠️ Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        # Run YOLO detection\n",
    "        results = model(frame, verbose=False)\n",
    "\n",
    "        # Annotate frame\n",
    "        annotated = results[0].plot()\n",
    "        cv2.imshow(\"ESP32-CAM + YOLOv8\", annotated)\n",
    "\n",
    "        # Process detections\n",
    "        for box in results[0].boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            conf = float(box.conf[0])\n",
    "            name = model.names[cls_id]\n",
    "\n",
    "            if name not in recent_detections:\n",
    "                if name in dangerous_objects:\n",
    "                    message = f\"Warning: {name} detected. Be careful.\"\n",
    "                elif name in obstacle_objects:\n",
    "                    message = f\"Obstacle ahead: {name}.\"\n",
    "                elif name in living_objects:\n",
    "                    message = f\"{name} nearby.\"\n",
    "                else:\n",
    "                    message = name  # Fallback\n",
    "\n",
    "                print(f\"🗣️ Saying: {message} (Confidence: {conf:.2f})\")\n",
    "                speak(message)\n",
    "                recent_detections.add(name)\n",
    "\n",
    "        # Reset detections every 5 seconds\n",
    "        if time.time() - last_reset_time > 5:\n",
    "            recent_detections.clear()\n",
    "            last_reset_time = time.time()\n",
    "\n",
    "        # Exit on 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e81a62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING torchvision==0.20 is incompatible with torch==2.6.\n",
      "Run 'pip install torchvision==0.21' to fix torchvision or 'pip install -U torch torchvision' to update both.\n",
      "For a full compatibility table see https://github.com/pytorch/vision#installation\n",
      "✅ Connected to ESP32-CAM stream\n",
      "🎥 Press 'q' to stop\n",
      "🗣️ tie (Confidence: 0.31)\n",
      "🗣️ person nearby. (Confidence: 0.26)\n",
      "🗣️ cat nearby. (Confidence: 0.32)\n",
      "🗣️ person nearby. (Confidence: 0.26)\n",
      "🗣️ cat nearby. (Confidence: 0.36)\n",
      "🗣️ tie (Confidence: 0.29)\n",
      "🗣️ Warning: dog detected. Be careful. (Confidence: 0.26)\n",
      "🗣️ cat nearby. (Confidence: 0.32)\n",
      "🗣️ Obstacle ahead: keyboard. (Confidence: 0.42)\n",
      "🗣️ Obstacle ahead: book. (Confidence: 0.31)\n",
      "🗣️ Obstacle ahead: laptop. (Confidence: 0.48)\n",
      "🗣️ Obstacle ahead: tv. (Confidence: 0.37)\n",
      "🗣️ Obstacle ahead: laptop. (Confidence: 0.36)\n",
      "🗣️ cat nearby. (Confidence: 0.25)\n",
      "🗣️ Obstacle ahead: chair. (Confidence: 0.39)\n",
      "🗣️ Obstacle ahead: laptop. (Confidence: 0.26)\n",
      "🗣️ Obstacle ahead: cell phone. (Confidence: 0.33)\n",
      "🗣️ person nearby. (Confidence: 0.37)\n",
      "🗣️ Obstacle ahead: laptop. (Confidence: 0.29)\n",
      "🗣️ Obstacle ahead: tv. (Confidence: 0.38)\n",
      "🗣️ Obstacle ahead: cell phone. (Confidence: 0.37)\n",
      "🗣️ cat nearby. (Confidence: 0.36)\n",
      "🗣️ person nearby. (Confidence: 0.31)\n",
      "🗣️ Obstacle ahead: book. (Confidence: 0.44)\n",
      "🗣️ Obstacle ahead: clock. (Confidence: 0.26)\n",
      "🗣️ tie (Confidence: 0.27)\n",
      "🗣️ cat nearby. (Confidence: 0.39)\n",
      "🗣️ person nearby. (Confidence: 0.28)\n",
      "🗣️ Warning: car detected. Be careful. (Confidence: 0.52)\n",
      "🗣️ cat nearby. (Confidence: 0.43)\n",
      "🗣️ person nearby. (Confidence: 0.28)\n",
      "🗣️ tie (Confidence: 0.55)\n",
      "🗣️ kite (Confidence: 0.29)\n",
      "🗣️ cat nearby. (Confidence: 0.38)\n",
      "🗣️ Obstacle ahead: book. (Confidence: 0.27)\n",
      "🗣️ cat nearby. (Confidence: 0.42)\n",
      "🗣️ Warning: car detected. Be careful. (Confidence: 0.35)\n",
      "🗣️ cat nearby. (Confidence: 0.46)\n",
      "🗣️ tie (Confidence: 0.30)\n",
      "🗣️ person nearby. (Confidence: 0.30)\n",
      "🗣️ cat nearby. (Confidence: 0.47)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import pyttsx3\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load model\n",
    "model = YOLO('yolov8m.pt')\n",
    "\n",
    "# Initialize TTS\n",
    "tts = pyttsx3.init()\n",
    "tts.setProperty('rate', 150)\n",
    "\n",
    "# ESP32-CAM stream URL\n",
    "stream_url = \"http://192.168.1.20:81/stream\"\n",
    "cap = cv2.VideoCapture(stream_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Failed to connect to ESP32-CAM stream\")\n",
    "else:\n",
    "    print(\"✅ Connected to ESP32-CAM stream\")\n",
    "\n",
    "# Object categories\n",
    "dangerous_objects = {\n",
    "    \"knife\", \"scissors\", \"fire hydrant\", \"train\", \"bus\", \"truck\", \"car\", \"motorcycle\",\n",
    "    \"bicycle\", \"airplane\", \"boat\", \"skateboard\", \"surfboard\", \"baseball bat\",\n",
    "    \"baseball glove\", \"tennis racket\", \"microwave\", \"oven\", \"toaster\", \"sink\",\n",
    "    \"refrigerator\", \"hair drier\", \"toilet\", \"bear\", \"dog\"\n",
    "}\n",
    "obstacle_objects = {\n",
    "    \"bench\", \"chair\", \"couch\", \"dining table\", \"bed\", \"tv\", \"vase\", \"potted plant\",\n",
    "    \"parking meter\", \"stop sign\", \"traffic light\", \"remote\", \"keyboard\", \"laptop\",\n",
    "    \"mouse\", \"cell phone\", \"book\", \"clock\", \"suitcase\", \"backpack\", \"handbag\", \"umbrella\"\n",
    "}\n",
    "living_objects = {\n",
    "    \"person\", \"dog\", \"cat\", \"bird\", \"horse\", \"sheep\", \"cow\", \"elephant\",\n",
    "    \"bear\", \"zebra\", \"giraffe\"\n",
    "}\n",
    "\n",
    "# Cooldown system: last spoken time per object class\n",
    "spoken_timestamps = {}\n",
    "cooldown_seconds = 10  # Speak again only after this time per object class\n",
    "\n",
    "def speak(text):\n",
    "    tts.say(text)\n",
    "    tts.runAndWait()\n",
    "\n",
    "if cap.isOpened():\n",
    "    print(\"🎥 Press 'q' to stop\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"⚠️ Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        results = model(frame, verbose=False)\n",
    "        annotated = results[0].plot()\n",
    "        cv2.imshow(\"ESP32-CAM + YOLOv8\", annotated)\n",
    "\n",
    "        current_time = time.time()\n",
    "        for box in results[0].boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            conf = float(box.conf[0])\n",
    "            name = model.names[cls_id]\n",
    "\n",
    "            # Check cooldown\n",
    "            last_spoken = spoken_timestamps.get(name, 0)\n",
    "            if current_time - last_spoken >= cooldown_seconds:\n",
    "                if name in dangerous_objects:\n",
    "                    message = f\"Warning: {name} detected. Be careful.\"\n",
    "                elif name in obstacle_objects:\n",
    "                    message = f\"Obstacle ahead: {name}.\"\n",
    "                elif name in living_objects:\n",
    "                    message = f\"{name} nearby.\"\n",
    "                else:\n",
    "                    message = name  # fallback\n",
    "\n",
    "                print(f\"🗣️ {message} (Confidence: {conf:.2f})\")\n",
    "                speak(message)\n",
    "                spoken_timestamps[name] = current_time\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa4323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0e579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d52e0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to ESP32-CAM stream\n",
      "🎥 Press 'q' to stop\n",
      "🗣️ Obstacle ahead: book. (Conf: 0.31)\n",
      "🗣️ cat nearby. (Conf: 0.37)\n",
      "🗣️ tie (Conf: 0.28)\n",
      "🗣️ person nearby. (Conf: 0.27)\n",
      "🗣️ cat nearby. (Conf: 0.30)\n",
      "🗣️ Warning: dog detected. Be careful. (Conf: 0.27)\n",
      "🗣️ Warning: car detected. Be careful. (Conf: 0.25)\n",
      "🗣️ bird nearby. (Conf: 0.31)\n",
      "🗣️ person nearby. (Conf: 0.26)\n",
      "🗣️ cat nearby. (Conf: 0.27)\n",
      "🗣️ Obstacle ahead: laptop. (Conf: 0.34)\n",
      "🗣️ Obstacle ahead: tv. (Conf: 0.35)\n",
      "🗣️ kite (Conf: 0.67)\n",
      "🗣️ Obstacle ahead: laptop. (Conf: 0.31)\n",
      "🗣️ cat nearby. (Conf: 0.28)\n",
      "🗣️ person nearby. (Conf: 0.27)\n",
      "🗣️ cat nearby. (Conf: 0.29)\n",
      "🗣️ Obstacle ahead: book. (Conf: 0.41)\n",
      "🗣️ cat nearby. (Conf: 0.25)\n",
      "🗣️ person nearby. (Conf: 0.38)\n",
      "🗣️ tie (Conf: 0.31)\n",
      "🗣️ bottle (Conf: 0.31)\n",
      "🗣️ Obstacle ahead: book. (Conf: 0.29)\n",
      "🗣️ tie (Conf: 0.27)\n",
      "🗣️ kite (Conf: 0.58)\n",
      "🗣️ bird nearby. (Conf: 0.25)\n",
      "🗣️ toothbrush (Conf: 0.55)\n",
      "🗣️ Warning: tennis racket detected. Be careful. (Conf: 0.72)\n",
      "🗣️ Obstacle ahead: laptop. (Conf: 0.40)\n",
      "🗣️ toothbrush (Conf: 0.43)\n",
      "⚠️ Failed to grab frame\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import pyttsx3\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8m.pt')\n",
    "\n",
    "# TTS setup\n",
    "tts = pyttsx3.init()\n",
    "tts.setProperty('rate', 150)\n",
    "\n",
    "# ESP32-CAM stream\n",
    "stream_url = \"http://192.168.1.20:81/stream\"\n",
    "cap = cv2.VideoCapture(stream_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Failed to connect to ESP32-CAM stream\")\n",
    "else:\n",
    "    print(\"✅ Connected to ESP32-CAM stream\")\n",
    "\n",
    "# Categories\n",
    "dangerous_objects = {\n",
    "    \"knife\", \"scissors\", \"fire hydrant\", \"train\", \"bus\", \"truck\", \"car\", \"motorcycle\",\n",
    "    \"bicycle\", \"airplane\", \"boat\", \"skateboard\", \"surfboard\", \"baseball bat\",\n",
    "    \"baseball glove\", \"tennis racket\", \"microwave\", \"oven\", \"toaster\", \"sink\",\n",
    "    \"refrigerator\", \"hair drier\", \"toilet\", \"bear\", \"dog\"\n",
    "}\n",
    "obstacle_objects = {\n",
    "    \"bench\", \"chair\", \"couch\", \"dining table\", \"bed\", \"tv\", \"vase\", \"potted plant\",\n",
    "    \"parking meter\", \"stop sign\", \"traffic light\", \"remote\", \"keyboard\", \"laptop\",\n",
    "    \"mouse\", \"cell phone\", \"book\", \"clock\", \"suitcase\", \"backpack\", \"handbag\", \"umbrella\"\n",
    "}\n",
    "living_objects = {\n",
    "    \"person\", \"dog\", \"cat\", \"bird\", \"horse\", \"sheep\", \"cow\", \"elephant\",\n",
    "    \"bear\", \"zebra\", \"giraffe\"\n",
    "}\n",
    "\n",
    "# Timers and history\n",
    "spoken_timestamps = {}\n",
    "object_locations = {}\n",
    "cooldown_seconds = 10\n",
    "movement_threshold = 50  # pixels\n",
    "prev_frame_classes = set()\n",
    "\n",
    "def speak(text):\n",
    "    tts.say(text)\n",
    "    tts.runAndWait()\n",
    "\n",
    "def get_center(box):\n",
    "    x1, y1, x2, y2 = box.xyxy[0]\n",
    "    return ((x1 + x2) / 2, (y1 + y2) / 2)\n",
    "\n",
    "if cap.isOpened():\n",
    "    print(\"🎥 Press 'q' to stop\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"⚠️ Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        results = model(frame, verbose=False)\n",
    "        annotated = results[0].plot()\n",
    "        cv2.imshow(\"ESP32-CAM + YOLOv8\", annotated)\n",
    "\n",
    "        current_time = time.time()\n",
    "        current_classes = set()\n",
    "\n",
    "        for box in results[0].boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            conf = float(box.conf[0])\n",
    "            name = model.names[cls_id]\n",
    "            current_classes.add(name)\n",
    "\n",
    "            # Get object center\n",
    "            center = get_center(box)\n",
    "\n",
    "            # Check movement\n",
    "            moved = True\n",
    "            if name in object_locations:\n",
    "                prev_x, prev_y = object_locations[name]\n",
    "                moved = (abs(center[0] - prev_x) > movement_threshold) or \\\n",
    "                        (abs(center[1] - prev_y) > movement_threshold)\n",
    "\n",
    "            object_locations[name] = center\n",
    "\n",
    "            # Speak only if:\n",
    "            # - Enough time passed (cooldown)\n",
    "            # - Object is new (not in prev frame)\n",
    "            # - Object moved significantly\n",
    "            last_spoken = spoken_timestamps.get(name, 0)\n",
    "            if (current_time - last_spoken >= cooldown_seconds and\n",
    "                name not in prev_frame_classes and\n",
    "                moved):\n",
    "\n",
    "                if name in dangerous_objects:\n",
    "                    message = f\"Warning: {name} detected. Be careful.\"\n",
    "                elif name in obstacle_objects:\n",
    "                    message = f\"Obstacle ahead: {name}.\"\n",
    "                elif name in living_objects:\n",
    "                    message = f\"{name} nearby.\"\n",
    "                else:\n",
    "                    message = name  # fallback\n",
    "\n",
    "                print(f\"🗣️ {message} (Conf: {conf:.2f})\")\n",
    "                speak(message)\n",
    "                spoken_timestamps[name] = current_time\n",
    "\n",
    "        # Update previous frame state\n",
    "        prev_frame_classes = current_classes\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b7e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9e18c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3040a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import pyttsx3\n",
    "import openai\n",
    "import io\n",
    "import base64\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# OpenAI API setup\n",
    "openai.api_key = 'your-api-key-here'  # Replace with your actual API key\n",
    "\n",
    "def speak(text):\n",
    "    tts.say(text)\n",
    "    tts.runAndWait()\n",
    "\n",
    "def convert_cv2_to_bytes(img):\n",
    "    _, buffer = cv2.imencode('.jpg', img)\n",
    "    return io.BytesIO(buffer.tobytes())\n",
    "\n",
    "def ask_gpt_about_image(image_bytes_io, user_prompt):\n",
    "    base64_img = base64.b64encode(image_bytes_io.getvalue()).decode('utf-8')\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-vision-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": user_prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{base64_img}\" }}\n",
    "            ]}\n",
    "        ],\n",
    "        max_tokens=200,\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Initialize YOLOv8 model\n",
    "model = YOLO('yolov8m.pt')\n",
    "\n",
    "# Initialize TTS\n",
    "tts = pyttsx3.init()\n",
    "tts.setProperty('rate', 150)\n",
    "\n",
    "# ESP32-CAM Stream URL\n",
    "stream_url = \"http://192.168.1.24:81/stream\"\n",
    "cap = cv2.VideoCapture(stream_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Failed to connect to ESP32-CAM stream\")\n",
    "    exit()\n",
    "else:\n",
    "    print(\"✅ Connected to ESP32-CAM stream\")\n",
    "\n",
    "# Dangerous or important objects\n",
    "high_priority = {\n",
    "    \"knife\", \"scissors\", \"fire hydrant\", \"bear\", \"truck\",\n",
    "    \"bus\", \"dog\", \"person\"\n",
    "}\n",
    "\n",
    "recent_detections = set()\n",
    "description_cache = {}\n",
    "gpt_last_called = {}\n",
    "gpt_cooldown = 30  # seconds\n",
    "last_reset_time = time.time()\n",
    "\n",
    "print(\"🎥 Press 'q' to quit\")\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"⚠️ Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    current_time = time.time()\n",
    "    results = model(frame, verbose=False)\n",
    "    annotated = results[0].plot()\n",
    "    cv2.imshow(\"ESP32-CAM + YOLOv8\", annotated)\n",
    "\n",
    "    for box in results[0].boxes:\n",
    "        cls_id = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        name = model.names[cls_id]\n",
    "\n",
    "        if name not in recent_detections:\n",
    "            print(f\"🔊 {name} detected ({conf:.2f})\")\n",
    "            speak(f\"{name} ahead\")\n",
    "            recent_detections.add(name)\n",
    "\n",
    "        # Handle GPT call for high-priority objects\n",
    "        if name in high_priority:\n",
    "            last_gpt = gpt_last_called.get(name, 0)\n",
    "            if current_time - last_gpt > gpt_cooldown:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                object_crop = frame[y1:y2, x1:x2]\n",
    "                img_bytes = convert_cv2_to_bytes(object_crop)\n",
    "\n",
    "                prompt = (\n",
    "                    f\"Describe this object to a blind person. Explain what it is and whether it is dangerous, in 1-2 simple sentences.\"\n",
    "                )\n",
    "                try:\n",
    "                    gpt_response = ask_gpt_about_image(img_bytes, prompt)\n",
    "                    print(f\"🧠 GPT: {gpt_response}\")\n",
    "                    speak(gpt_response)\n",
    "                    gpt_last_called[name] = current_time\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ GPT error: {e}\")\n",
    "\n",
    "    if time.time() - last_reset_time > 5:\n",
    "        recent_detections.clear()\n",
    "        last_reset_time = time.time()\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b9e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d42f9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8l.pt to 'yolov8l.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83.7M/83.7M [01:00<00:00, 1.46MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to ESP32-CAM stream at http://192.168.1.20:81/stream\n",
      "\n",
      "🎥 Press 'q' to quit the stream.\n",
      "Waiting for objects...\n",
      "🔊 tie detected (0.58)\n",
      "🔊 person detected (0.27)\n",
      "🔊 tie detected (0.53)\n",
      "🔊 person detected (0.45)\n",
      "🔊 person detected (0.49)\n",
      "🔊 tie detected (0.45)\n",
      "🔊 tie detected (0.52)\n",
      "🔊 person detected (0.38)\n",
      "🔊 tie detected (0.57)\n",
      "🔊 person detected (0.37)\n",
      "🔊 tie detected (0.57)\n",
      "🔊 person detected (0.38)\n",
      "🔊 tie detected (0.48)\n",
      "🔊 person detected (0.42)\n",
      "🔊 person detected (0.50)\n",
      "🔊 cat detected (0.26)\n",
      "Calling Gemini for: person (conf: 0.65)...\n",
      "🧠 Gemini: The image shows a person, likely a man, in a dimly lit room;  there is no immediate danger present, but exercise caution as the context is unclear.\n",
      "\n",
      "🔊 tie detected (0.26)\n",
      "🔊 person detected (0.63)\n",
      "🔊 cat detected (0.32)\n",
      "⚠️ Failed to grab frame. Reconnecting or ending stream.\n",
      "🔊 person detected (0.53)\n",
      "🔊 person detected (0.62)\n",
      "🔊 cat detected (0.31)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 155\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recent_detections:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🔊 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m detected (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconf\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[43mspeak\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m ahead\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m     recent_detections.add(name) \u001b[38;5;66;03m# Add to the set of recently announced objects\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# 2. Handle Gemini call for high-priority objects\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mspeak\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Converts text to speech and plays it.\"\"\"\u001b[39;00m\n\u001b[32m     30\u001b[39m tts.say(text)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43mtts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunAndWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\Desktop\\Projects\\recom_env\\Lib\\site-packages\\pyttsx3\\engine.py:183\u001b[39m, in \u001b[36mEngine.runAndWait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mself\u001b[39m._inLoop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m._driverLoop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunAndWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\Desktop\\Projects\\recom_env\\Lib\\site-packages\\pyttsx3\\driver.py:195\u001b[39m, in \u001b[36mDriverProxy.runAndWait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mCalled by the engine to start an event loop, process all commands in\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03mthe queue at the start of the loop, and then exit the loop.\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m    194\u001b[39m \u001b[38;5;28mself\u001b[39m._push(\u001b[38;5;28mself\u001b[39m._engine.endLoop, \u001b[38;5;28mtuple\u001b[39m())\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_driver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstartLoop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asus\\Desktop\\Projects\\recom_env\\Lib\\site-packages\\pyttsx3\\drivers\\sapi5.py:146\u001b[39m, in \u001b[36mSAPI5Driver.startLoop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    144\u001b[39m     first = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    145\u001b[39m pythoncom.PumpWaitingMessages()\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m time.sleep(\u001b[32m0.05\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import pyttsx3\n",
    "import io\n",
    "import base64\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import google.generativeai as genai\n",
    "import os # For accessing environment variables\n",
    "\n",
    "# --- Google Gemini API setup ---\n",
    "# It's highly recommended to set your API key as an environment variable named GOOGLE_API_KEY.\n",
    "# The `genai.configure` function will automatically pick it up.\n",
    "try:\n",
    "    genai.configure(api_key=\"AIzaSyCpv933505daIyqD6jX4ApPcj5U0CXOJGA\")\n",
    "except KeyError:\n",
    "    print(\"❌ GOOGLE_API_KEY environment variable not set.\")\n",
    "    print(\"Please set the GOOGLE_API_KEY environment variable or hardcode it in the script (not recommended).\")\n",
    "    # You can uncomment the line below and replace 'YOUR_API_KEY_HERE' if you must hardcode it.\n",
    "    # genai.configure(api_key=\"YOUR_API_KEY_HERE\")\n",
    "    exit() # Exit if API key is not configured\n",
    "\n",
    "# Optional: Configure generation parameters for Gemini\n",
    "# Adjust temperature for creativity (0.0 for deterministic, 1.0 for very creative)\n",
    "# Other parameters like top_p, top_k can also be set.\n",
    "gemini_generation_config = genai.types.GenerationConfig(temperature=0.4, max_output_tokens=200)\n",
    "\n",
    "def speak(text):\n",
    "    \"\"\"Converts text to speech and plays it.\"\"\"\n",
    "    tts.say(text)\n",
    "    tts.runAndWait()\n",
    "\n",
    "def convert_cv2_to_bytes(img):\n",
    "    \"\"\"Converts an OpenCV image (NumPy array) to a BytesIO object.\"\"\"\n",
    "    _, buffer = cv2.imencode('.jpg', img)\n",
    "    return io.BytesIO(buffer.tobytes())\n",
    "\n",
    "def ask_gemini_about_image(image_bytes_io, user_prompt):\n",
    "    \"\"\"\n",
    "    Sends an image and a text prompt to Google Gemini Pro Vision and returns the response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert BytesIO to PIL Image, which Gemini's API prefers\n",
    "        pil_image = Image.open(image_bytes_io)\n",
    "\n",
    "        # Initialize the Gemini Vision model\n",
    "        # 'gemini-1.5-flash-latest' is generally faster and more cost-effective for vision tasks\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "        # Create the content list for the multimodal prompt (text first, then image)\n",
    "        contents = [user_prompt, pil_image]\n",
    "\n",
    "        # Generate content from the model with specified generation config\n",
    "        response = model.generate_content(contents, generation_config=gemini_generation_config)\n",
    "\n",
    "        return response.text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error communicating with Gemini: {e}\")\n",
    "        return \"Sorry, I couldn't get a detailed description right now.\"\n",
    "\n",
    "# Initialize YOLOv8 model\n",
    "# 'yolov8m.pt' is the medium model, good balance of speed and accuracy.\n",
    "model = YOLO('yolov8l.pt')\n",
    "\n",
    "# Initialize Text-to-Speech engine\n",
    "tts = pyttsx3.init()\n",
    "tts.setProperty('rate', 170) # Adjust speech rate (words per minute)\n",
    "\n",
    "# ESP32-CAM Stream URL\n",
    "# IMPORTANT: Replace with the actual IP address of your ESP32-CAM\n",
    "stream_url = \"http://192.168.1.20:81/stream\"\n",
    "cap = cv2.VideoCapture(stream_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"❌ Failed to connect to ESP32-CAM stream at {stream_url}\")\n",
    "    print(\"Please check the IP address and ensure the ESP32-CAM is powered on and streaming.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"✅ Connected to ESP32-CAM stream at {stream_url}\")\n",
    "\n",
    "# Define dangerous or important objects that warrant a detailed Gemini description\n",
    "high_priority_objects = {\n",
    "    \"knife\", \"scissors\", \"fire hydrant\", \"bear\", \"truck\",\n",
    "    \"bus\", \"dog\", \"person\", \"cat\", \"car\", \"bicycle\", \"motorcycle\", \"train\"\n",
    "}\n",
    "\n",
    "# Pre-defined simple descriptions for high-priority objects when Gemini is on cooldown\n",
    "local_high_priority_descriptions = {\n",
    "    \"knife\": \"Warning, a sharp knife is detected.\",\n",
    "    \"scissors\": \"Be careful, scissors are nearby.\",\n",
    "    \"person\": \"A person is present.\",\n",
    "    \"dog\": \"A dog is detected.\",\n",
    "    \"cat\": \"A cat is detected.\",\n",
    "    \"car\": \"A car is detected.\",\n",
    "    \"truck\": \"A truck is detected.\",\n",
    "    \"bus\": \"A bus is detected.\",\n",
    "    \"bicycle\": \"A bicycle is detected.\",\n",
    "    \"motorcycle\": \"A motorcycle is detected.\",\n",
    "    \"train\": \"A train is detected.\",\n",
    "    \"fire hydrant\": \"A fire hydrant is detected.\",\n",
    "    \"bear\": \"Warning, a bear is detected. Proceed with caution.\",\n",
    "}\n",
    "\n",
    "\n",
    "# --- Cooldown and Detection Management ---\n",
    "recent_detections = set() # Stores objects announced by YOLO in the last reset period\n",
    "gemini_last_called = {}   # Stores the last time Gemini was called for a specific object\n",
    "global_gemini_last_called_time = 0 # Stores the last time any Gemini call was made\n",
    "\n",
    "gemini_cooldown_per_object = 30 # seconds - How long to wait before describing the same object again\n",
    "global_gemini_cooldown = 10     # seconds - How long to wait before any new Gemini call\n",
    "\n",
    "# Confidence threshold for YOLO detection before even considering Gemini\n",
    "yolo_confidence_threshold_for_gemini = 0.65 # Only ask Gemini if YOLO is at least this confident\n",
    "\n",
    "# How often to clear `recent_detections` to allow re-announcements\n",
    "recent_detection_reset_interval = 5 # seconds\n",
    "last_reset_time = time.time()\n",
    "\n",
    "print(\"\\n🎥 Press 'q' to quit the stream.\")\n",
    "print(\"Waiting for objects...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"⚠️ Failed to grab frame. Reconnecting or ending stream.\")\n",
    "        # Attempt to reconnect or break\n",
    "        cap.release()\n",
    "        time.sleep(2) # Wait a bit before retrying\n",
    "        cap = cv2.VideoCapture(stream_url)\n",
    "        if not cap.isOpened():\n",
    "            break # Exit if reconnection fails\n",
    "        continue\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Perform object detection with YOLOv8\n",
    "    results = model(frame, verbose=False) # verbose=False to suppress verbose output\n",
    "    annotated_frame = results[0].plot() # Draws bounding boxes and labels on the frame\n",
    "    cv2.imshow(\"ESP32-CAM + YOLOv8\", annotated_frame)\n",
    "\n",
    "    current_frame_detections = set() # Objects detected in *this* specific frame\n",
    "\n",
    "    for box in results[0].boxes:\n",
    "        cls_id = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        name = model.names[cls_id]\n",
    "\n",
    "        current_frame_detections.add(name) # Add to current frame's detections\n",
    "\n",
    "        # 1. Initial YOLO announcement for new detections\n",
    "        if name not in recent_detections:\n",
    "            print(f\"🔊 {name} detected ({conf:.2f})\")\n",
    "            speak(f\"{name} ahead\")\n",
    "            recent_detections.add(name) # Add to the set of recently announced objects\n",
    "\n",
    "        # 2. Handle Gemini call for high-priority objects\n",
    "        if name in high_priority_objects:\n",
    "            # Check if YOLO's confidence is high enough for a Gemini call\n",
    "            if conf < yolo_confidence_threshold_for_gemini:\n",
    "                # print(f\"DEBUG: {name} confidence too low ({conf:.2f}) for Gemini call.\")\n",
    "                continue # Skip Gemini call if confidence is low\n",
    "\n",
    "            # Check global Gemini cooldown\n",
    "            if current_time - global_gemini_last_called_time < global_gemini_cooldown:\n",
    "                # print(f\"DEBUG: Global Gemini cooldown active. Skipping {name} description.\")\n",
    "                # If global cooldown is active, but it's a high-priority object,\n",
    "                # and we haven't given a basic announcement yet, give one.\n",
    "                # This part is mostly covered by the `recent_detections` check above,\n",
    "                # but adds robustness if the initial YOLO announcement was missed for some reason.\n",
    "                if name not in recent_detections:\n",
    "                    if name in local_high_priority_descriptions:\n",
    "                        speak(local_high_priority_descriptions[name])\n",
    "                        recent_detections.add(name) # Consider it announced if local fallback is used\n",
    "                continue # Skip Gemini call\n",
    "\n",
    "            # Check per-object Gemini cooldown\n",
    "            last_call_for_this_object = gemini_last_called.get(name, 0)\n",
    "            if current_time - last_call_for_this_object > gemini_cooldown_per_object:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                # Ensure crop coordinates are within frame bounds to avoid errors\n",
    "                x1, y1, x2, y2 = max(0, x1), max(0, y1), min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "\n",
    "                # Only proceed if the cropped area is valid\n",
    "                if (x2 - x1) > 0 and (y2 - y1) > 0:\n",
    "                    object_crop = frame[y1:y2, x1:x2]\n",
    "                    img_bytes = convert_cv2_to_bytes(object_crop)\n",
    "\n",
    "                    prompt = (\n",
    "                        f\"Describe this specific detected object to a blind person. \"\n",
    "                        f\"Explain what it is and whether it is dangerous or something to be aware of. \"\n",
    "                        f\"Provide the description in 1-2 simple, concise sentences.\"\n",
    "                    )\n",
    "                    try:\n",
    "                        print(f\"Calling Gemini for: {name} (conf: {conf:.2f})...\")\n",
    "                        gemini_response = ask_gemini_about_image(img_bytes, prompt)\n",
    "                        print(f\"🧠 Gemini: {gemini_response}\")\n",
    "                        speak(gemini_response)\n",
    "                        gemini_last_called[name] = current_time # Update per-object call time\n",
    "                        global_gemini_last_called_time = current_time # Update global call time\n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Gemini API call error: {e}\")\n",
    "                        # Fallback to local description if API call fails\n",
    "                        if name in local_high_priority_descriptions:\n",
    "                            speak(local_high_priority_descriptions[name])\n",
    "                else:\n",
    "                    print(f\"DEBUG: Skipping Gemini for {name}, invalid crop area.\")\n",
    "            else:\n",
    "                # Object in per-object cooldown, use local description if not recently announced (handled by recent_detections)\n",
    "                # print(f\"DEBUG: {name} in per-object cooldown. Skipping Gemini.\")\n",
    "                pass # Already handled by initial YOLO announcement and global cooldown checks\n",
    "\n",
    "    # Periodically clear `recent_detections` to allow re-announcements of persistent objects\n",
    "    if time.time() - last_reset_time > recent_detection_reset_interval:\n",
    "        recent_detections.clear()\n",
    "        last_reset_time = time.time()\n",
    "        # print(\"DEBUG: recent_detections cleared.\")\n",
    "\n",
    "\n",
    "    # Exit loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"\\nExiting application.\")\n",
    "        break\n",
    "\n",
    "# Release video capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "tts.stop() # Ensure TTS engine is properly shut down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ebf3fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # This will read .env and set os.environ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd10d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 00:09:15,133 - INFO - Gemini API configured successfully\n",
      "2025-06-07 00:09:16,021 - INFO - YOLO model loaded with 80 classes\n",
      "2025-06-07 00:09:16,022 - INFO - Thread pools initialized with 3 workers\n",
      "2025-06-07 00:09:16,025 - INFO - TTS engine initialized successfully\n",
      "2025-06-07 00:09:16,156 - INFO - Connected to stream: http://192.168.1.20:81/stream\n",
      "2025-06-07 00:09:16,156 - INFO - Starting vision assistance - Target FPS: 30\n",
      "2025-06-07 00:09:16,156 - INFO - Press 'q' to quit\n",
      "2025-06-07 00:09:16,788 - INFO - New detection: person (confidence: 0.30)\n",
      "2025-06-07 00:09:16,971 - INFO - New detection: cat (confidence: 0.30)\n",
      "2025-06-07 00:09:20,268 - INFO - New detection: tie (confidence: 0.28)\n",
      "2025-06-07 00:09:21,255 - INFO - New detection: person (confidence: 0.26)\n",
      "2025-06-07 00:09:21,420 - INFO - New detection: potted plant (confidence: 0.30)\n",
      "2025-06-07 00:09:30,904 - INFO - New detection: book (confidence: 0.28)\n",
      "2025-06-07 00:09:31,904 - INFO - New detection: cat (confidence: 0.40)\n",
      "2025-06-07 00:09:32,168 - INFO - New detection: book (confidence: 0.29)\n",
      "2025-06-07 00:09:36,921 - INFO - New detection: cat (confidence: 0.29)\n",
      "2025-06-07 00:09:37,291 - INFO - New detection: book (confidence: 0.61)\n",
      "2025-06-07 00:09:41,690 - INFO - New detection: cat (confidence: 0.30)\n",
      "2025-06-07 00:09:42,638 - INFO - New detection: book (confidence: 0.62)\n",
      "2025-06-07 00:09:46,622 - INFO - New detection: book (confidence: 0.26)\n",
      "2025-06-07 00:09:46,754 - INFO - New detection: cat (confidence: 0.52)\n",
      "2025-06-07 00:09:52,391 - INFO - New detection: cat (confidence: 0.30)\n",
      "2025-06-07 00:09:57,572 - INFO - New detection: cat (confidence: 0.25)\n",
      "2025-06-07 00:10:01,272 - INFO - New detection: tie (confidence: 0.27)\n",
      "2025-06-07 00:10:01,801 - INFO - New detection: person (confidence: 0.25)\n",
      "2025-06-07 00:10:03,054 - INFO - New detection: person (confidence: 0.36)\n",
      "2025-06-07 00:10:03,188 - INFO - New detection: cat (confidence: 0.40)\n",
      "2025-06-07 00:10:07,804 - INFO - New detection: cat (confidence: 0.39)\n",
      "2025-06-07 00:10:13,221 - INFO - New detection: cat (confidence: 0.27)\n",
      "2025-06-07 00:10:18,088 - INFO - New detection: cat (confidence: 0.29)\n",
      "2025-06-07 00:10:23,254 - INFO - New detection: person (confidence: 0.32)\n",
      "2025-06-07 00:10:23,508 - INFO - New detection: cat (confidence: 0.29)\n",
      "2025-06-07 00:10:25,404 - INFO - New detection: tie (confidence: 0.29)\n",
      "2025-06-07 00:10:25,787 - INFO - New detection: bed (confidence: 0.39)\n",
      "2025-06-07 00:10:25,788 - INFO - New detection: laptop (confidence: 0.34)\n",
      "2025-06-07 00:10:27,754 - INFO - New detection: laptop (confidence: 0.44)\n",
      "2025-06-07 00:10:27,754 - INFO - New detection: bed (confidence: 0.28)\n",
      "2025-06-07 00:10:30,982 - INFO - New detection: car (confidence: 0.49)\n",
      "2025-06-07 00:10:32,570 - INFO - New detection: laptop (confidence: 0.58)\n",
      "2025-06-07 00:10:32,571 - INFO - New detection: bed (confidence: 0.28)\n",
      "2025-06-07 00:10:33,087 - INFO - New detection: fire hydrant (confidence: 0.43)\n",
      "2025-06-07 00:10:33,088 - INFO - New detection: car (confidence: 0.31)\n",
      "2025-06-07 00:10:33,713 - INFO - New detection: tv (confidence: 0.30)\n",
      "2025-06-07 00:10:37,619 - INFO - New detection: laptop (confidence: 0.46)\n",
      "2025-06-07 00:10:37,621 - INFO - New detection: tv (confidence: 0.34)\n",
      "2025-06-07 00:10:37,753 - INFO - New detection: bed (confidence: 0.27)\n",
      "2025-06-07 00:10:43,123 - INFO - New detection: laptop (confidence: 0.29)\n",
      "2025-06-07 00:10:44,787 - INFO - New detection: tie (confidence: 0.53)\n",
      "2025-06-07 00:10:46,367 - INFO - New detection: cat (confidence: 0.33)\n",
      "2025-06-07 00:10:48,405 - INFO - New detection: cat (confidence: 0.36)\n",
      "2025-06-07 00:10:53,421 - INFO - New detection: tie (confidence: 0.25)\n",
      "2025-06-07 00:10:56,088 - INFO - New detection: cat (confidence: 0.29)\n",
      "2025-06-07 00:11:00,971 - INFO - New detection: tie (confidence: 0.31)\n",
      "2025-06-07 00:11:04,538 - INFO - New detection: cat (confidence: 0.30)\n",
      "2025-06-07 00:11:12,023 - INFO - New detection: tie (confidence: 0.31)\n",
      "2025-06-07 00:11:12,921 - INFO - New detection: person (confidence: 0.32)\n",
      "2025-06-07 00:11:15,166 - INFO - New detection: cat (confidence: 0.29)\n",
      "2025-06-07 00:11:17,187 - INFO - New detection: person (confidence: 0.25)\n",
      "2025-06-07 00:11:18,088 - INFO - New detection: tie (confidence: 0.26)\n",
      "2025-06-07 00:11:20,105 - INFO - New detection: tie (confidence: 0.32)\n",
      "2025-06-07 00:11:20,598 - INFO - New detection: cat (confidence: 0.29)\n",
      "2025-06-07 00:11:21,598 - INFO - New detection: person (confidence: 0.27)\n",
      "2025-06-07 00:11:25,798 - INFO - New detection: cat (confidence: 0.28)\n",
      "2025-06-07 00:11:26,183 - INFO - New detection: tie (confidence: 0.33)\n",
      "2025-06-07 00:11:27,071 - INFO - New detection: person (confidence: 0.27)\n",
      "2025-06-07 00:11:30,538 - INFO - New detection: person (confidence: 0.36)\n",
      "2025-06-07 00:11:30,668 - INFO - New detection: cat (confidence: 0.38)\n",
      "2025-06-07 00:11:31,421 - INFO - New detection: tie (confidence: 0.26)\n",
      "2025-06-07 00:11:32,354 - INFO - Requesting Gemini description for: person\n",
      "2025-06-07 00:11:32,604 - INFO - New detection: suitcase (confidence: 0.42)\n",
      "2025-06-07 00:11:32,988 - INFO - New detection: bottle (confidence: 0.27)\n",
      "2025-06-07 00:11:35,471 - INFO - Gemini response for person: The image shows a dark-haired, possibly unshaven man with glasses, shirtless and wearing denim overalls; the lighting is poor, making it difficult to assess his immediate surroundings and potential safety hazards.\n",
      "2025-06-07 00:11:35,571 - INFO - New detection: person (confidence: 0.31)\n",
      "2025-06-07 00:11:37,650 - INFO - New detection: tie (confidence: 0.40)\n",
      "2025-06-07 00:11:37,654 - INFO - New detection: bottle (confidence: 0.32)\n",
      "2025-06-07 00:11:38,834 - INFO - New detection: dog (confidence: 0.30)\n",
      "2025-06-07 00:11:40,655 - INFO - New detection: person (confidence: 0.39)\n",
      "2025-06-07 00:11:41,304 - INFO - New detection: cat (confidence: 0.26)\n",
      "2025-06-07 00:11:45,955 - INFO - New detection: cat (confidence: 0.26)\n",
      "2025-06-07 00:11:46,871 - INFO - Quit command received\n",
      "2025-06-07 00:11:46,871 - INFO - Starting cleanup process...\n",
      "2025-06-07 00:11:46,873 - ERROR - Cleanup error: ThreadPoolExecutor.shutdown() got an unexpected keyword argument 'timeout'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import pyttsx3\n",
    "import io\n",
    "import gc\n",
    "import threading\n",
    "import logging\n",
    "import queue\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Set, Optional, List\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration class for easy parameter management\"\"\"\n",
    "    YOLO_MODEL: str = 'yolov8m.pt'\n",
    "    SPEECH_RATE: int = 170\n",
    "    GEMINI_COOLDOWN: int = 10\n",
    "    OBJECT_COOLDOWN: int = 30\n",
    "    CONFIDENCE_THRESHOLD: float = 0.65\n",
    "    STREAM_URL: str = \"http://192.168.1.20:81/stream\"\n",
    "    DETECTION_RESET_INTERVAL: int = 5\n",
    "    PROCESS_EVERY_N_FRAMES: int = 2\n",
    "    MAX_GEMINI_RETRIES: int = 3\n",
    "    GEMINI_TIMEOUT: int = 10\n",
    "    TARGET_FPS: int = 30\n",
    "    MAX_RECONNECTION_ATTEMPTS: int = 5\n",
    "    THREAD_POOL_SIZE: int = 3\n",
    "    MEMORY_CLEANUP_INTERVAL: int = 100  # frames\n",
    "    HIGH_PRIORITY_OBJECTS: List[str] = field(default_factory=lambda: [\n",
    "        \"knife\", \"scissors\", \"fire hydrant\", \"bear\", \"truck\",\n",
    "        \"bus\", \"dog\", \"person\", \"cat\", \"car\", \"bicycle\", \n",
    "        \"motorcycle\", \"train\", \"stop sign\", \"traffic light\"\n",
    "    ])\n",
    "\n",
    "class ResourceManager:\n",
    "    \"\"\"Manages system resources and cleanup\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.frame_count = 0\n",
    "        self.memory_cleanup_interval = 100\n",
    "        \n",
    "    def should_cleanup_memory(self) -> bool:\n",
    "        \"\"\"Determine if memory cleanup should be performed\"\"\"\n",
    "        self.frame_count += 1\n",
    "        return self.frame_count % self.memory_cleanup_interval == 0\n",
    "    \n",
    "    def cleanup_memory(self):\n",
    "        \"\"\"Force garbage collection and memory cleanup\"\"\"\n",
    "        try:\n",
    "            gc.collect()\n",
    "            logger.debug(f\"Memory cleanup performed at frame {self.frame_count}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Memory cleanup failed: {e}\")\n",
    "\n",
    "class TTSManager:\n",
    "    \"\"\"Manages Text-to-Speech with proper resource handling\"\"\"\n",
    "    \n",
    "    def __init__(self, speech_rate: int = 170):\n",
    "        self.speech_rate = speech_rate\n",
    "        self.tts_queue = queue.Queue()\n",
    "        self.is_running = True\n",
    "        self.tts_thread = None\n",
    "        self._init_tts()\n",
    "        \n",
    "    def _init_tts(self):\n",
    "        \"\"\"Initialize TTS engine with error handling\"\"\"\n",
    "        try:\n",
    "            self.tts = pyttsx3.init()\n",
    "            self.tts.setProperty('rate', self.speech_rate)\n",
    "            \n",
    "            # Start TTS worker thread\n",
    "            self.tts_thread = threading.Thread(target=self._tts_worker, daemon=True)\n",
    "            self.tts_thread.start()\n",
    "            logger.info(\"TTS engine initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize TTS: {e}\")\n",
    "            self.tts = None\n",
    "    \n",
    "    def _tts_worker(self):\n",
    "        \"\"\"Worker thread for TTS operations\"\"\"\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                text = self.tts_queue.get(timeout=1.0)\n",
    "                if text is None:  # Shutdown signal\n",
    "                    break\n",
    "                if self.tts:\n",
    "                    self.tts.say(text)\n",
    "                    self.tts.runAndWait()\n",
    "                self.tts_queue.task_done()\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                logger.error(f\"TTS worker error: {e}\")\n",
    "    \n",
    "    def speak_async(self, text: str):\n",
    "        \"\"\"Add text to TTS queue for asynchronous speech\"\"\"\n",
    "        if self.is_running and not self.tts_queue.full():\n",
    "            try:\n",
    "                self.tts_queue.put_nowait(text)\n",
    "            except queue.Full:\n",
    "                logger.warning(\"TTS queue full, skipping speech\")\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Properly cleanup TTS resources\"\"\"\n",
    "        try:\n",
    "            self.is_running = False\n",
    "            self.tts_queue.put(None)  # Shutdown signal\n",
    "            \n",
    "            if self.tts_thread and self.tts_thread.is_alive():\n",
    "                self.tts_thread.join(timeout=2.0)\n",
    "            \n",
    "            if self.tts and hasattr(self.tts, 'stop'):\n",
    "                try:\n",
    "                    self.tts.stop()\n",
    "                except:\n",
    "                    pass  # Ignore stop() failures\n",
    "                    \n",
    "            logger.info(\"TTS cleanup completed\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"TTS cleanup error: {e}\")\n",
    "\n",
    "class FPSController:\n",
    "    \"\"\"Controls frame processing rate\"\"\"\n",
    "    \n",
    "    def __init__(self, target_fps: int = 30):\n",
    "        self.target_fps = target_fps\n",
    "        self.frame_time = 1.0 / target_fps\n",
    "        self.last_frame_time = time.time()\n",
    "    \n",
    "    def wait_for_next_frame(self):\n",
    "        \"\"\"Wait to maintain target FPS\"\"\"\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - self.last_frame_time\n",
    "        \n",
    "        if elapsed < self.frame_time:\n",
    "            time.sleep(self.frame_time - elapsed)\n",
    "        \n",
    "        self.last_frame_time = time.time()\n",
    "\n",
    "class SmartVisionAssistant:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.setup_gemini()\n",
    "        self.setup_models()\n",
    "        self.setup_tracking()\n",
    "        self.setup_executors()\n",
    "        self.setup_managers()\n",
    "        \n",
    "    def setup_gemini(self):\n",
    "        \"\"\"Initialize Gemini API with proper error handling\"\"\"\n",
    "        api_key = os.getenv('GOOGLE_API_KEY')\n",
    "        if not api_key:\n",
    "            logger.error(\"GOOGLE_API_KEY environment variable not set\")\n",
    "            raise ValueError(\"API key not configured\")\n",
    "        \n",
    "        try:\n",
    "            genai.configure(api_key=api_key)\n",
    "            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "            self.generation_config = genai.types.GenerationConfig(\n",
    "                temperature=0.4, \n",
    "                max_output_tokens=200\n",
    "            )\n",
    "            logger.info(\"Gemini API configured successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to configure Gemini API: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def setup_models(self):\n",
    "        \"\"\"Initialize YOLO model with validation\"\"\"\n",
    "        try:\n",
    "            self.yolo_model = YOLO(self.config.YOLO_MODEL)\n",
    "            \n",
    "            # Validate model names\n",
    "            if hasattr(self.yolo_model, 'names') and self.yolo_model.names:\n",
    "                self.model_names = self.yolo_model.names\n",
    "                logger.info(f\"YOLO model loaded with {len(self.model_names)} classes\")\n",
    "            else:\n",
    "                logger.warning(\"YOLO model has no class names, using fallback\")\n",
    "                self.model_names = {i: f\"class_{i}\" for i in range(80)}  # COCO default\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load YOLO model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def setup_tracking(self):\n",
    "        \"\"\"Initialize tracking variables\"\"\"\n",
    "        self.recent_detections: Set[str] = set()\n",
    "        self.gemini_last_called: Dict[str, float] = {}\n",
    "        self.global_gemini_last_called_time = 0\n",
    "        self.last_reset_time = time.time()\n",
    "        self.frame_count = 0\n",
    "        self.reconnection_attempts = 0\n",
    "        \n",
    "        # Use configurable high-priority objects\n",
    "        self.high_priority_objects = set(self.config.HIGH_PRIORITY_OBJECTS)\n",
    "        \n",
    "        self.local_descriptions = {\n",
    "            \"knife\": \"Warning: Sharp knife detected. Exercise caution.\",\n",
    "            \"scissors\": \"Scissors present. Handle with care.\",\n",
    "            \"person\": \"Person detected in the area.\",\n",
    "            \"dog\": \"Dog spotted nearby.\",\n",
    "            \"cat\": \"Cat detected in the vicinity.\",\n",
    "            \"car\": \"Vehicle present - stay alert.\",\n",
    "            \"truck\": \"Large truck detected.\",\n",
    "            \"bus\": \"Bus in the area.\",\n",
    "            \"bicycle\": \"Bicycle detected.\",\n",
    "            \"motorcycle\": \"Motorcycle present.\",\n",
    "            \"train\": \"Train detected - maintain safe distance.\",\n",
    "            \"fire hydrant\": \"Fire hydrant located nearby.\",\n",
    "            \"bear\": \"DANGER: Bear detected. Move to safety immediately.\",\n",
    "            \"stop sign\": \"Stop sign ahead.\",\n",
    "            \"traffic light\": \"Traffic light detected.\"\n",
    "        }\n",
    "    \n",
    "    def setup_executors(self):\n",
    "        \"\"\"Initialize thread pools with controlled limits\"\"\"\n",
    "        self.gemini_executor = ThreadPoolExecutor(\n",
    "            max_workers=self.config.THREAD_POOL_SIZE,\n",
    "            thread_name_prefix=\"GeminiWorker\"\n",
    "        )\n",
    "        logger.info(f\"Thread pools initialized with {self.config.THREAD_POOL_SIZE} workers\")\n",
    "    \n",
    "    def setup_managers(self):\n",
    "        \"\"\"Initialize resource managers\"\"\"\n",
    "        self.tts_manager = TTSManager(self.config.SPEECH_RATE)\n",
    "        self.fps_controller = FPSController(self.config.TARGET_FPS)\n",
    "        self.resource_manager = ResourceManager()\n",
    "    \n",
    "    def get_object_name(self, cls_id: int) -> str:\n",
    "        \"\"\"Safely get object name with fallback\"\"\"\n",
    "        try:\n",
    "            return self.model_names.get(cls_id, f\"unknown_class_{cls_id}\")\n",
    "        except Exception:\n",
    "            return f\"class_{cls_id}\"\n",
    "    \n",
    "    @contextmanager\n",
    "    def safe_image_processing(self, image_data):\n",
    "        \"\"\"Context manager for safe image processing with cleanup\"\"\"\n",
    "        pil_image = None\n",
    "        try:\n",
    "            if isinstance(image_data, io.BytesIO):\n",
    "                image_data.seek(0)\n",
    "                pil_image = Image.open(image_data)\n",
    "            yield pil_image\n",
    "        finally:\n",
    "            if pil_image:\n",
    "                pil_image.close()\n",
    "            if isinstance(image_data, io.BytesIO):\n",
    "                image_data.close()\n",
    "    \n",
    "    def convert_cv2_to_bytes(self, img) -> io.BytesIO:\n",
    "        \"\"\"Convert OpenCV image to BytesIO with error handling\"\"\"\n",
    "        try:\n",
    "            _, buffer = cv2.imencode('.jpg', img, [cv2.IMWRITE_JPEG_QUALITY, 85])\n",
    "            return io.BytesIO(buffer.tobytes())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Image conversion error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def ask_gemini_about_image(self, image_bytes_io: io.BytesIO, prompt: str) -> str:\n",
    "        \"\"\"Robust Gemini API call with proper resource management\"\"\"\n",
    "        for attempt in range(self.config.MAX_GEMINI_RETRIES):\n",
    "            try:\n",
    "                with self.safe_image_processing(image_bytes_io) as pil_image:\n",
    "                    if not pil_image:\n",
    "                        return \"Image processing failed\"\n",
    "                    \n",
    "                    contents = [prompt, pil_image]\n",
    "                    response = self.gemini_model.generate_content(\n",
    "                        contents, \n",
    "                        generation_config=self.generation_config\n",
    "                    )\n",
    "                    \n",
    "                    if response and response.text:\n",
    "                        return response.text.strip()\n",
    "                    else:\n",
    "                        logger.warning(f\"Empty Gemini response on attempt {attempt + 1}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Gemini API attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == self.config.MAX_GEMINI_RETRIES - 1:\n",
    "                    return \"Unable to get detailed description at this time.\"\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        \n",
    "        return \"Description service temporarily unavailable.\"\n",
    "    \n",
    "    def connect_to_stream(self) -> cv2.VideoCapture:\n",
    "        \"\"\"Establish connection to video stream with retry limits\"\"\"\n",
    "        for attempt in range(self.config.MAX_RECONNECTION_ATTEMPTS):\n",
    "            try:\n",
    "                cap = cv2.VideoCapture(self.config.STREAM_URL)\n",
    "                if cap.isOpened():\n",
    "                    # Test if we can actually read a frame\n",
    "                    ret, _ = cap.read()\n",
    "                    if ret:\n",
    "                        logger.info(f\"Connected to stream: {self.config.STREAM_URL}\")\n",
    "                        self.reconnection_attempts = 0  # Reset counter on success\n",
    "                        return cap\n",
    "                    else:\n",
    "                        cap.release()\n",
    "                        logger.warning(f\"Stream connected but no frames available (attempt {attempt + 1})\")\n",
    "                else:\n",
    "                    logger.warning(f\"Stream connection attempt {attempt + 1} failed\")\n",
    "                    \n",
    "                time.sleep(min(2 ** attempt, 10))  # Exponential backoff with cap\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Stream connection error on attempt {attempt + 1}: {e}\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        self.reconnection_attempts += 1\n",
    "        if self.reconnection_attempts >= self.config.MAX_RECONNECTION_ATTEMPTS:\n",
    "            raise ConnectionError(f\"Failed to connect to stream after {self.config.MAX_RECONNECTION_ATTEMPTS} attempts. Giving up.\")\n",
    "        else:\n",
    "            raise ConnectionError(f\"Failed to connect to stream, attempt {self.reconnection_attempts}\")\n",
    "    \n",
    "    def process_detections(self, frame, results):\n",
    "        \"\"\"Process YOLO detection results\"\"\"\n",
    "        current_time = time.time()\n",
    "        current_frame_detections = set()\n",
    "        \n",
    "        for box in results[0].boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            conf = float(box.conf[0])\n",
    "            name = self.get_object_name(cls_id)\n",
    "            \n",
    "            current_frame_detections.add(name)\n",
    "            \n",
    "            # Initial YOLO announcement\n",
    "            if name not in self.recent_detections:\n",
    "                logger.info(f\"New detection: {name} (confidence: {conf:.2f})\")\n",
    "                self.tts_manager.speak_async(f\"{name} detected\")\n",
    "                self.recent_detections.add(name)\n",
    "            \n",
    "            # Handle high-priority objects\n",
    "            if name in self.high_priority_objects:\n",
    "                self.handle_high_priority_object(frame, box, name, conf, current_time)\n",
    "    \n",
    "    def handle_high_priority_object(self, frame, box, name: str, conf: float, current_time: float):\n",
    "        \"\"\"Handle high-priority object detection with controlled threading\"\"\"\n",
    "        # Check confidence threshold\n",
    "        if conf < self.config.CONFIDENCE_THRESHOLD:\n",
    "            return\n",
    "        \n",
    "        # Check global cooldown\n",
    "        if current_time - self.global_gemini_last_called_time < self.config.GEMINI_COOLDOWN:\n",
    "            return\n",
    "        \n",
    "        # Check per-object cooldown\n",
    "        last_call = self.gemini_last_called.get(name, 0)\n",
    "        if current_time - last_call <= self.config.OBJECT_COOLDOWN:\n",
    "            return\n",
    "        \n",
    "        # Extract object region safely\n",
    "        try:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "            \n",
    "            if (x2 - x1) <= 0 or (y2 - y1) <= 0:\n",
    "                return\n",
    "            \n",
    "            # Create a copy to avoid race conditions\n",
    "            object_crop = frame[y1:y2, x1:x2].copy()\n",
    "            \n",
    "            # Submit to thread pool (non-blocking)\n",
    "            future = self.gemini_executor.submit(\n",
    "                self.process_with_gemini,\n",
    "                object_crop, name, current_time\n",
    "            )\n",
    "            \n",
    "            # Update timestamps immediately to prevent duplicate calls\n",
    "            self.gemini_last_called[name] = current_time\n",
    "            self.global_gemini_last_called_time = current_time\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error handling high-priority object {name}: {e}\")\n",
    "    \n",
    "    def process_with_gemini(self, object_crop, name: str, timestamp: float):\n",
    "        \"\"\"Process object with Gemini API (runs in thread pool)\"\"\"\n",
    "        img_bytes = None\n",
    "        try:\n",
    "            img_bytes = self.convert_cv2_to_bytes(object_crop)\n",
    "            prompt = (\n",
    "                f\"Describe this {name} to a visually impaired person. \"\n",
    "                f\"Focus on safety considerations and important details. \"\n",
    "                f\"Keep it concise - 1-2 sentences maximum.\"\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Requesting Gemini description for: {name}\")\n",
    "            gemini_response = self.ask_gemini_about_image(img_bytes, prompt)\n",
    "            \n",
    "            if gemini_response and \"unavailable\" not in gemini_response.lower():\n",
    "                logger.info(f\"Gemini response for {name}: {gemini_response}\")\n",
    "                self.tts_manager.speak_async(gemini_response)\n",
    "            else:\n",
    "                logger.warning(f\"Gemini failed for {name}, using fallback: {gemini_response}\")\n",
    "                # Fallback to local description\n",
    "                fallback = self.local_descriptions.get(name, f\"{name} detected\")\n",
    "                self.tts_manager.speak_async(fallback)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gemini processing error for {name}: {e}\")\n",
    "            # Use local fallback\n",
    "            fallback = self.local_descriptions.get(name, f\"{name} detected\")\n",
    "            self.tts_manager.speak_async(fallback)\n",
    "        finally:\n",
    "            # Cleanup\n",
    "            if img_bytes:\n",
    "                img_bytes.close()\n",
    "            del object_crop  # Explicit cleanup\n",
    "    \n",
    "    def should_process_frame(self) -> bool:\n",
    "        \"\"\"Determine if current frame should be processed\"\"\"\n",
    "        self.frame_count += 1\n",
    "        return self.frame_count % self.config.PROCESS_EVERY_N_FRAMES == 0\n",
    "    \n",
    "    def reset_recent_detections(self):\n",
    "        \"\"\"Periodically reset recent detections\"\"\"\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_reset_time > self.config.DETECTION_RESET_INTERVAL:\n",
    "            self.recent_detections.clear()\n",
    "            self.last_reset_time = current_time\n",
    "            logger.debug(\"Recent detections cleared\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main application loop with comprehensive error handling\"\"\"\n",
    "        cap = None\n",
    "        try:\n",
    "            cap = self.connect_to_stream()\n",
    "            logger.info(f\"Starting vision assistance - Target FPS: {self.config.TARGET_FPS}\")\n",
    "            logger.info(\"Press 'q' to quit\")\n",
    "            \n",
    "            while True:\n",
    "                # Control frame rate\n",
    "                self.fps_controller.wait_for_next_frame()\n",
    "                \n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    logger.warning(\"Failed to grab frame, attempting reconnection\")\n",
    "                    cap.release()\n",
    "                    time.sleep(2)\n",
    "                    try:\n",
    "                        cap = self.connect_to_stream()\n",
    "                        continue\n",
    "                    except ConnectionError as e:\n",
    "                        logger.error(f\"Reconnection failed: {e}\")\n",
    "                        break\n",
    "                \n",
    "                # Process frame selectively\n",
    "                if self.should_process_frame():\n",
    "                    try:\n",
    "                        results = self.yolo_model(frame, verbose=False)\n",
    "                        self.process_detections(frame, results)\n",
    "                        \n",
    "                        # Display annotated frame\n",
    "                        annotated_frame = results[0].plot()\n",
    "                        cv2.imshow(\"Smart Vision Assistant\", annotated_frame)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Frame processing error: {e}\")\n",
    "                \n",
    "                # Periodic maintenance\n",
    "                self.reset_recent_detections()\n",
    "                \n",
    "                # Memory cleanup\n",
    "                if self.resource_manager.should_cleanup_memory():\n",
    "                    self.resource_manager.cleanup_memory()\n",
    "                \n",
    "                # Check for quit command\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    logger.info(\"Quit command received\")\n",
    "                    break\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Application interrupted by user\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {e}\")\n",
    "        finally:\n",
    "            self.cleanup(cap)\n",
    "    \n",
    "    def cleanup(self, cap):\n",
    "        \"\"\"Comprehensive resource cleanup\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting cleanup process...\")\n",
    "            \n",
    "            # Close video capture\n",
    "            if cap and cap.isOpened():\n",
    "                cap.release()\n",
    "            \n",
    "            # Cleanup OpenCV windows\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            # Shutdown executors\n",
    "            if hasattr(self, 'gemini_executor'):\n",
    "                self.gemini_executor.shutdown(wait=True, timeout=5.0)\n",
    "            \n",
    "            # Cleanup TTS\n",
    "            if hasattr(self, 'tts_manager'):\n",
    "                self.tts_manager.cleanup()\n",
    "            \n",
    "            # Force final garbage collection\n",
    "            gc.collect()\n",
    "            \n",
    "            logger.info(\"Cleanup completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cleanup error: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point with configuration validation\"\"\"\n",
    "    try:\n",
    "        config = Config()\n",
    "        \n",
    "        # Validate critical configuration\n",
    "        if not os.path.exists(config.YOLO_MODEL) and not config.YOLO_MODEL.startswith('yolov8'):\n",
    "            logger.warning(f\"YOLO model file not found: {config.YOLO_MODEL}\")\n",
    "        \n",
    "        assistant = SmartVisionAssistant(config)\n",
    "        assistant.run()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Application failed to start: {e}\")\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    exit(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf57ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING torchvision==0.20 is incompatible with torch==2.6.\n",
      "Run 'pip install torchvision==0.21' to fix torchvision or 'pip install -U torch torchvision' to update both.\n",
      "For a full compatibility table see https://github.com/pytorch/vision#installation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 00:20:56,356 - INFO - Gemini API configured successfully\n",
      "2025-06-07 00:20:56,539 - INFO - YOLO model yolov8l.pt loaded successfully\n",
      "2025-06-07 00:20:56,604 - INFO - Imported existing <module 'comtypes.gen' from 'c:\\\\Users\\\\asus\\\\Desktop\\\\Projects\\\\recom_env\\\\Lib\\\\site-packages\\\\comtypes\\\\gen\\\\__init__.py'>\n",
      "2025-06-07 00:20:56,605 - INFO - Using writeable comtypes cache directory: 'c:\\Users\\asus\\Desktop\\Projects\\recom_env\\Lib\\site-packages\\comtypes\\gen'\n",
      "2025-06-07 00:20:56,709 - INFO - TTS engine initialized\n",
      "2025-06-07 00:20:56,773 - INFO - Connected to stream: http://192.168.1.20:81/stream\n",
      "2025-06-07 00:20:56,774 - INFO - Starting vision assistance - Press 'q' to quit\n",
      "2025-06-07 00:21:00,253 - INFO - New detection: person (confidence: 0.45)\n",
      "2025-06-07 00:21:00,269 - INFO - New detection: cat (confidence: 0.44)\n",
      "2025-06-07 00:21:00,269 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:01,955 - INFO - New detection: person (confidence: 0.67)\n",
      "2025-06-07 00:21:01,955 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:01,961 - INFO - Requesting Gemini description for: person\n",
      "2025-06-07 00:21:02,255 - INFO - New detection: cat (confidence: 0.27)\n",
      "2025-06-07 00:21:02,255 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:05,272 - INFO - Gemini response: The image shows a dark-skinned man with dark hair, wearing a dark-colored sleeveless top; the lighting is poor, making it difficult to assess his surroundings or potential safety hazards.\n",
      "\n",
      "2025-06-07 00:21:07,630 - INFO - New detection: person (confidence: 0.56)\n",
      "2025-06-07 00:21:07,630 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:09,115 - INFO - New detection: cat (confidence: 0.26)\n",
      "2025-06-07 00:21:09,115 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:13,625 - INFO - New detection: person (confidence: 0.47)\n",
      "2025-06-07 00:21:13,625 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:15,121 - INFO - New detection: cat (confidence: 0.25)\n",
      "2025-06-07 00:21:15,121 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:18,890 - INFO - New detection: cat (confidence: 0.38)\n",
      "2025-06-07 00:21:20,116 - INFO - New detection: person (confidence: 0.30)\n",
      "2025-06-07 00:21:20,116 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:23,772 - INFO - New detection: cat (confidence: 0.27)\n",
      "2025-06-07 00:21:25,439 - INFO - New detection: laptop (confidence: 0.94)\n",
      "2025-06-07 00:21:25,439 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:25,439 - INFO - New detection: bed (confidence: 0.49)\n",
      "2025-06-07 00:21:25,439 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:28,872 - INFO - New detection: laptop (confidence: 0.77)\n",
      "2025-06-07 00:21:29,021 - INFO - New detection: bed (confidence: 0.31)\n",
      "2025-06-07 00:21:29,028 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:33,905 - INFO - New detection: laptop (confidence: 0.67)\n",
      "2025-06-07 00:21:34,062 - INFO - New detection: bed (confidence: 0.26)\n",
      "2025-06-07 00:21:34,062 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:36,375 - INFO - New detection: cat (confidence: 0.28)\n",
      "2025-06-07 00:21:39,457 - INFO - New detection: bed (confidence: 0.27)\n",
      "2025-06-07 00:21:43,372 - INFO - New detection: cat (confidence: 0.39)\n",
      "2025-06-07 00:21:44,021 - INFO - New detection: bed (confidence: 0.27)\n",
      "2025-06-07 00:21:44,029 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:46,771 - INFO - New detection: cat (confidence: 0.31)\n",
      "2025-06-07 00:21:47,242 - INFO - New detection: person (confidence: 0.55)\n",
      "2025-06-07 00:21:47,242 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:47,871 - INFO - New detection: bottle (confidence: 0.37)\n",
      "2025-06-07 00:21:47,880 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:50,955 - INFO - New detection: cat (confidence: 0.47)\n",
      "2025-06-07 00:21:52,879 - INFO - New detection: bed (confidence: 0.26)\n",
      "2025-06-07 00:21:52,879 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:54,038 - INFO - New detection: cat (confidence: 0.26)\n",
      "2025-06-07 00:21:55,838 - INFO - New detection: person (confidence: 0.41)\n",
      "2025-06-07 00:21:55,846 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:21:59,303 - INFO - New detection: cat (confidence: 0.37)\n",
      "2025-06-07 00:22:07,405 - INFO - New detection: person (confidence: 0.25)\n",
      "2025-06-07 00:22:09,099 - INFO - New detection: cat (confidence: 0.26)\n",
      "2025-06-07 00:22:09,100 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:09,591 - INFO - New detection: cat (confidence: 0.26)\n",
      "2025-06-07 00:22:09,592 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:09,905 - INFO - New detection: person (confidence: 0.30)\n",
      "2025-06-07 00:22:12,058 - INFO - New detection: chair (confidence: 0.34)\n",
      "2025-06-07 00:22:12,058 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:14,755 - INFO - New detection: person (confidence: 0.31)\n",
      "2025-06-07 00:22:19,005 - INFO - New detection: chair (confidence: 0.36)\n",
      "2025-06-07 00:22:20,044 - INFO - New detection: chair (confidence: 0.33)\n",
      "2025-06-07 00:22:20,044 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:20,707 - INFO - New detection: person (confidence: 0.30)\n",
      "2025-06-07 00:22:20,707 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:21,797 - INFO - New detection: keyboard (confidence: 0.26)\n",
      "2025-06-07 00:22:22,124 - INFO - New detection: laptop (confidence: 0.46)\n",
      "2025-06-07 00:22:22,124 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:23,968 - INFO - New detection: bed (confidence: 0.44)\n",
      "2025-06-07 00:22:23,971 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:24,484 - INFO - New detection: bed (confidence: 0.28)\n",
      "2025-06-07 00:22:24,486 - INFO - New detection: laptop (confidence: 0.27)\n",
      "2025-06-07 00:22:24,487 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:25,339 - INFO - New detection: chair (confidence: 0.26)\n",
      "2025-06-07 00:22:25,340 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:25,674 - INFO - New detection: cat (confidence: 0.36)\n",
      "2025-06-07 00:22:25,674 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:29,472 - INFO - New detection: cat (confidence: 0.60)\n",
      "2025-06-07 00:22:29,821 - INFO - New detection: person (confidence: 0.52)\n",
      "2025-06-07 00:22:29,821 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:31,421 - INFO - New detection: vase (confidence: 0.41)\n",
      "2025-06-07 00:22:31,425 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:32,493 - INFO - Requesting Gemini description for: cat\n",
      "2025-06-07 00:22:32,673 - INFO - Requesting Gemini description for: person\n",
      "2025-06-07 00:22:33,021 - INFO - Requesting Gemini description for: person\n",
      "2025-06-07 00:22:33,205 - INFO - Requesting Gemini description for: person\n",
      "2025-06-07 00:22:33,372 - INFO - Requesting Gemini description for: person\n",
      "2025-06-07 00:22:33,555 - INFO - New detection: laptop (confidence: 0.38)\n",
      "2025-06-07 00:22:33,555 - INFO - Requesting Gemini description for: person\n",
      "2025-06-07 00:22:33,722 - INFO - Requesting Gemini description for: person\n",
      "2025-06-07 00:22:34,253 - INFO - Requesting Gemini description for: person\n",
      "2025-06-07 00:22:34,323 - INFO - Gemini response: That's not a cat; that's a photo of a person's chest and face.  There is no cat present in the image, so there are no safety considerations related to a cat.\n",
      "\n",
      "2025-06-07 00:22:34,328 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:34,424 - INFO - New detection: dog (confidence: 0.32)\n",
      "2025-06-07 00:22:34,424 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:34,606 - INFO - New detection: person (confidence: 0.83)\n",
      "2025-06-07 00:22:34,606 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:34,775 - INFO - New detection: laptop (confidence: 0.32)\n",
      "2025-06-07 00:22:34,777 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:35,755 - INFO - Gemini response: He appears to be a man with dark hair and a beard, wearing a gray tank top and glasses;  exercise caution as the image quality is poor and doesn't provide sufficient detail for a complete assessment.\n",
      "\n",
      "2025-06-07 00:22:35,755 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:35,838 - INFO - New detection: cat (confidence: 0.30)\n",
      "2025-06-07 00:22:35,838 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:35,905 - INFO - Gemini response: He appears to be a man with dark hair and a beard, wearing a gray tank top and clear glasses;  the image quality is poor, so details are unclear, and caution is advised if interacting in person.\n",
      "\n",
      "2025-06-07 00:22:36,010 - INFO - New detection: book (confidence: 0.27)\n",
      "2025-06-07 00:22:36,010 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:36,206 - INFO - Gemini response: He appears to be a man with dark hair and a beard, wearing glasses and a sleeveless shirt;  the image quality is poor, so details are unclear, making it difficult to assess any immediate safety concerns.\n",
      "\n",
      "2025-06-07 00:22:36,207 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:36,607 - INFO - Gemini response: He appears to be a man with dark, curly hair and a beard, wearing a dark gray tank top;  note the presence of what looks like an earbud wire, possibly indicating headphones.\n",
      "\n",
      "2025-06-07 00:22:36,607 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:36,890 - INFO - New detection: dog (confidence: 0.31)\n",
      "2025-06-07 00:22:36,892 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:37,145 - INFO - Gemini response: He appears to be a man with dark, curly hair and a beard, wearing a gray tank top;  the image quality is poor, so details are limited, impacting accurate safety assessment.\n",
      "\n",
      "2025-06-07 00:22:37,145 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:37,523 - INFO - Gemini response: He appears to be a man with dark, somewhat unkempt hair and a beard, wearing a sleeveless shirt;  the image quality is poor, making it difficult to assess any immediate safety concerns.\n",
      "\n",
      "2025-06-07 00:22:37,524 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:39,005 - INFO - Gemini response: He appears to be a man with dark, somewhat unkempt hair, a beard, and glasses; he's wearing a sleeveless shirt, so be mindful of potential sun exposure if outdoors.\n",
      "\n",
      "2025-06-07 00:22:39,014 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:39,707 - INFO - New detection: person (confidence: 0.79)\n",
      "2025-06-07 00:22:39,707 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:40,055 - INFO - New detection: cat (confidence: 0.36)\n",
      "2025-06-07 00:22:40,055 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:40,059 - INFO - New detection: book (confidence: 0.35)\n",
      "2025-06-07 00:22:40,059 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:40,405 - INFO - New detection: laptop (confidence: 0.37)\n",
      "2025-06-07 00:22:40,405 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:44,755 - INFO - New detection: person (confidence: 0.78)\n",
      "2025-06-07 00:22:44,755 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:45,464 - INFO - New detection: cat (confidence: 0.34)\n",
      "2025-06-07 00:22:45,464 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:45,826 - INFO - New detection: book (confidence: 0.26)\n",
      "2025-06-07 00:22:45,826 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:46,893 - INFO - New detection: dog (confidence: 0.35)\n",
      "2025-06-07 00:22:46,894 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:49,741 - INFO - New detection: person (confidence: 0.64)\n",
      "2025-06-07 00:22:49,741 - INFO - New detection: cat (confidence: 0.37)\n",
      "2025-06-07 00:22:49,741 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:51,177 - INFO - New detection: dog (confidence: 0.34)\n",
      "2025-06-07 00:22:51,177 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:54,805 - INFO - New detection: cat (confidence: 0.35)\n",
      "2025-06-07 00:22:54,972 - INFO - New detection: person (confidence: 0.52)\n",
      "2025-06-07 00:22:54,972 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:55,884 - INFO - New detection: dog (confidence: 0.60)\n",
      "2025-06-07 00:22:55,888 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:22:59,358 - INFO - New detection: laptop (confidence: 0.26)\n",
      "2025-06-07 00:23:00,099 - INFO - New detection: dog (confidence: 0.46)\n",
      "2025-06-07 00:23:00,101 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:00,103 - INFO - New detection: cat (confidence: 0.38)\n",
      "2025-06-07 00:23:00,104 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:01,000 - INFO - New detection: mouse (confidence: 0.29)\n",
      "2025-06-07 00:23:01,002 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:02,104 - INFO - New detection: laptop (confidence: 0.30)\n",
      "2025-06-07 00:23:02,832 - INFO - New detection: person (confidence: 0.49)\n",
      "2025-06-07 00:23:02,832 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:03,929 - INFO - Requesting Gemini description for: cat\n",
      "2025-06-07 00:23:04,493 - INFO - New detection: bed (confidence: 0.32)\n",
      "2025-06-07 00:23:04,494 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:05,022 - INFO - New detection: dog (confidence: 0.54)\n",
      "2025-06-07 00:23:05,022 - INFO - New detection: person (confidence: 0.36)\n",
      "2025-06-07 00:23:05,022 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:05,563 - INFO - Requesting Gemini description for: person\n",
      "2025-06-07 00:23:05,739 - INFO - New detection: cat (confidence: 0.38)\n",
      "2025-06-07 00:23:05,739 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:05,849 - INFO - Gemini response: That's not a cat; that's a photo of a person.  There is no cat in the image to describe.\n",
      "\n",
      "2025-06-07 00:23:05,855 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:05,921 - INFO - New detection: book (confidence: 0.32)\n",
      "2025-06-07 00:23:05,921 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:07,605 - INFO - Gemini response: The person appears to be a male with dark, curly hair and a beard, wearing a light-colored tank top;  the image quality is poor, making it difficult to assess any immediate safety concerns.\n",
      "\n",
      "2025-06-07 00:23:08,257 - INFO - New detection: mouse (confidence: 0.31)\n",
      "2025-06-07 00:23:08,257 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:10,043 - INFO - New detection: cat (confidence: 0.33)\n",
      "2025-06-07 00:23:10,054 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:10,605 - INFO - New detection: keyboard (confidence: 0.28)\n",
      "2025-06-07 00:23:10,610 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:10,963 - INFO - New detection: person (confidence: 0.30)\n",
      "2025-06-07 00:23:10,964 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:14,855 - INFO - New detection: mouse (confidence: 0.69)\n",
      "2025-06-07 00:23:14,855 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:15,043 - INFO - New detection: cat (confidence: 0.67)\n",
      "2025-06-07 00:23:15,043 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:15,043 - INFO - New detection: mouse (confidence: 0.36)\n",
      "2025-06-07 00:23:15,048 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:15,048 - INFO - New detection: person (confidence: 0.26)\n",
      "2025-06-07 00:23:15,053 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:18,913 - INFO - New detection: dog (confidence: 0.33)\n",
      "2025-06-07 00:23:18,917 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:20,229 - INFO - New detection: cat (confidence: 0.42)\n",
      "2025-06-07 00:23:20,229 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:23:20,235 - INFO - Quit command received\n",
      "2025-06-07 00:23:20,492 - INFO - Cleanup completed\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import pyttsx3\n",
    "import io\n",
    "import threading\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Set, Optional\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration class for easy parameter management\"\"\"\n",
    "    YOLO_MODEL: str = 'yolov8l.pt'\n",
    "    SPEECH_RATE: int = 170\n",
    "    GEMINI_COOLDOWN: int = 10\n",
    "    OBJECT_COOLDOWN: int = 30\n",
    "    CONFIDENCE_THRESHOLD: float = 0.65\n",
    "    STREAM_URL: str = \"http://192.168.1.20:81/stream\"\n",
    "    DETECTION_RESET_INTERVAL: int = 5\n",
    "    PROCESS_EVERY_N_FRAMES: int = 2\n",
    "    MAX_GEMINI_RETRIES: int = 3\n",
    "    GEMINI_TIMEOUT: int = 10\n",
    "\n",
    "class SmartVisionAssistant:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.setup_gemini()\n",
    "        self.setup_models()\n",
    "        self.setup_tracking()\n",
    "        \n",
    "    def setup_gemini(self):\n",
    "        \"\"\"Initialize Gemini API with proper error handling\"\"\"\n",
    "        api_key = os.getenv('GOOGLE_API_KEY')\n",
    "        if not api_key:\n",
    "            logger.error(\"GOOGLE_API_KEY environment variable not set\")\n",
    "            raise ValueError(\"API key not configured\")\n",
    "        \n",
    "        try:\n",
    "            genai.configure(api_key=api_key)\n",
    "            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "            self.generation_config = genai.types.GenerationConfig(\n",
    "                temperature=0.4, \n",
    "                max_output_tokens=200\n",
    "            )\n",
    "            logger.info(\"Gemini API configured successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to configure Gemini API: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def setup_models(self):\n",
    "        \"\"\"Initialize YOLO model and TTS engine\"\"\"\n",
    "        try:\n",
    "            self.yolo_model = YOLO(self.config.YOLO_MODEL)\n",
    "            logger.info(f\"YOLO model {self.config.YOLO_MODEL} loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load YOLO model: {e}\")\n",
    "            raise\n",
    "        \n",
    "        try:\n",
    "            self.tts = pyttsx3.init()\n",
    "            self.tts.setProperty('rate', self.config.SPEECH_RATE)\n",
    "            logger.info(\"TTS engine initialized\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize TTS: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def setup_tracking(self):\n",
    "        \"\"\"Initialize tracking variables\"\"\"\n",
    "        self.recent_detections: Set[str] = set()\n",
    "        self.gemini_last_called: Dict[str, float] = {}\n",
    "        self.global_gemini_last_called_time = 0\n",
    "        self.last_reset_time = time.time()\n",
    "        self.frame_count = 0\n",
    "        \n",
    "        # Enhanced object categories\n",
    "        self.high_priority_objects = {\n",
    "            \"knife\", \"scissors\", \"fire hydrant\", \"bear\", \"truck\",\n",
    "            \"bus\", \"dog\", \"person\", \"cat\", \"car\", \"bicycle\", \n",
    "            \"motorcycle\", \"train\", \"stop sign\", \"traffic light\"\n",
    "        }\n",
    "        \n",
    "        self.local_descriptions = {\n",
    "            \"knife\": \"Warning: Sharp knife detected. Exercise caution.\",\n",
    "            \"scissors\": \"Scissors present. Handle with care.\",\n",
    "            \"person\": \"Person detected in the area.\",\n",
    "            \"dog\": \"Dog spotted nearby.\",\n",
    "            \"cat\": \"Cat detected in the vicinity.\",\n",
    "            \"car\": \"Vehicle present - stay alert.\",\n",
    "            \"truck\": \"Large truck detected.\",\n",
    "            \"bus\": \"Bus in the area.\",\n",
    "            \"bicycle\": \"Bicycle detected.\",\n",
    "            \"motorcycle\": \"Motorcycle present.\",\n",
    "            \"train\": \"Train detected - maintain safe distance.\",\n",
    "            \"fire hydrant\": \"Fire hydrant located nearby.\",\n",
    "            \"bear\": \"DANGER: Bear detected. Move to safety immediately.\",\n",
    "            \"stop sign\": \"Stop sign ahead.\",\n",
    "            \"traffic light\": \"Traffic light detected.\"\n",
    "        }\n",
    "    \n",
    "    def speak_async(self, text: str):\n",
    "        \"\"\"Non-blocking text-to-speech\"\"\"\n",
    "        def _speak():\n",
    "            try:\n",
    "                self.tts.say(text)\n",
    "                self.tts.runAndWait()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"TTS error: {e}\")\n",
    "        \n",
    "        threading.Thread(target=_speak, daemon=True).start()\n",
    "    \n",
    "    def convert_cv2_to_bytes(self, img) -> io.BytesIO:\n",
    "        \"\"\"Convert OpenCV image to BytesIO with error handling\"\"\"\n",
    "        try:\n",
    "            _, buffer = cv2.imencode('.jpg', img, [cv2.IMWRITE_JPEG_QUALITY, 85])\n",
    "            return io.BytesIO(buffer.tobytes())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Image conversion error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def ask_gemini_about_image(self, image_bytes_io: io.BytesIO, prompt: str) -> str:\n",
    "        \"\"\"Robust Gemini API call with retries\"\"\"\n",
    "        for attempt in range(self.config.MAX_GEMINI_RETRIES):\n",
    "            try:\n",
    "                image_bytes_io.seek(0)  # Reset buffer position\n",
    "                pil_image = Image.open(image_bytes_io)\n",
    "                \n",
    "                contents = [prompt, pil_image]\n",
    "                response = self.gemini_model.generate_content(\n",
    "                    contents, \n",
    "                    generation_config=self.generation_config\n",
    "                )\n",
    "                \n",
    "                # Clean up PIL image\n",
    "                pil_image.close()\n",
    "                return response.text\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Gemini API attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == self.config.MAX_GEMINI_RETRIES - 1:\n",
    "                    return \"Unable to get detailed description at this time.\"\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        \n",
    "        return \"Description service temporarily unavailable.\"\n",
    "    \n",
    "    def connect_to_stream(self) -> cv2.VideoCapture:\n",
    "        \"\"\"Establish connection to video stream with retries\"\"\"\n",
    "        max_attempts = 3\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                cap = cv2.VideoCapture(self.config.STREAM_URL)\n",
    "                if cap.isOpened():\n",
    "                    logger.info(f\"Connected to stream: {self.config.STREAM_URL}\")\n",
    "                    return cap\n",
    "                else:\n",
    "                    logger.warning(f\"Stream connection attempt {attempt + 1} failed\")\n",
    "                    time.sleep(2)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Stream connection error: {e}\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        raise ConnectionError(f\"Failed to connect to stream after {max_attempts} attempts\")\n",
    "    \n",
    "    def process_detections(self, frame, results):\n",
    "        \"\"\"Process YOLO detection results\"\"\"\n",
    "        current_time = time.time()\n",
    "        current_frame_detections = set()\n",
    "        \n",
    "        for box in results[0].boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            conf = float(box.conf[0])\n",
    "            name = self.yolo_model.names[cls_id]\n",
    "            \n",
    "            current_frame_detections.add(name)\n",
    "            \n",
    "            # Initial YOLO announcement\n",
    "            if name not in self.recent_detections:\n",
    "                logger.info(f\"New detection: {name} (confidence: {conf:.2f})\")\n",
    "                self.speak_async(f\"{name} detected\")\n",
    "                self.recent_detections.add(name)\n",
    "            \n",
    "            # Handle high-priority objects\n",
    "            if name in self.high_priority_objects:\n",
    "                self.handle_high_priority_object(frame, box, name, conf, current_time)\n",
    "    \n",
    "    def handle_high_priority_object(self, frame, box, name: str, conf: float, current_time: float):\n",
    "        \"\"\"Handle high-priority object detection with Gemini integration\"\"\"\n",
    "        # Check confidence threshold\n",
    "        if conf < self.config.CONFIDENCE_THRESHOLD:\n",
    "            return\n",
    "        \n",
    "        # Check global cooldown\n",
    "        if current_time - self.global_gemini_last_called_time < self.config.GEMINI_COOLDOWN:\n",
    "            return\n",
    "        \n",
    "        # Check per-object cooldown\n",
    "        last_call = self.gemini_last_called.get(name, 0)\n",
    "        if current_time - last_call <= self.config.OBJECT_COOLDOWN:\n",
    "            return\n",
    "        \n",
    "        # Extract object region\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "        \n",
    "        if (x2 - x1) <= 0 or (y2 - y1) <= 0:\n",
    "            return\n",
    "        \n",
    "        # Process with Gemini in separate thread\n",
    "        object_crop = frame[y1:y2, x1:x2].copy()\n",
    "        threading.Thread(\n",
    "            target=self.process_with_gemini,\n",
    "            args=(object_crop, name, current_time),\n",
    "            daemon=True\n",
    "        ).start()\n",
    "    \n",
    "    def process_with_gemini(self, object_crop, name: str, timestamp: float):\n",
    "        \"\"\"Process object with Gemini API (runs in separate thread)\"\"\"\n",
    "        try:\n",
    "            img_bytes = self.convert_cv2_to_bytes(object_crop)\n",
    "            prompt = (\n",
    "                f\"Describe this {name} to a visually impaired person. \"\n",
    "                f\"Focus on safety considerations and important details. \"\n",
    "                f\"Keep it concise - 1-2 sentences maximum.\"\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Requesting Gemini description for: {name}\")\n",
    "            gemini_response = self.ask_gemini_about_image(img_bytes, prompt)\n",
    "            \n",
    "            if gemini_response and \"unavailable\" not in gemini_response.lower():\n",
    "                logger.info(f\"Gemini response: {gemini_response}\")\n",
    "                self.speak_async(gemini_response)\n",
    "                self.gemini_last_called[name] = timestamp\n",
    "                self.global_gemini_last_called_time = timestamp\n",
    "            else:\n",
    "                # Fallback to local description\n",
    "                fallback = self.local_descriptions.get(name, f\"{name} detected\")\n",
    "                self.speak_async(fallback)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gemini processing error for {name}: {e}\")\n",
    "            # Use local fallback\n",
    "            fallback = self.local_descriptions.get(name, f\"{name} detected\")\n",
    "            self.speak_async(fallback)\n",
    "    \n",
    "    def should_process_frame(self) -> bool:\n",
    "        \"\"\"Determine if current frame should be processed\"\"\"\n",
    "        self.frame_count += 1\n",
    "        return self.frame_count % self.config.PROCESS_EVERY_N_FRAMES == 0\n",
    "    \n",
    "    def reset_recent_detections(self):\n",
    "        \"\"\"Periodically reset recent detections\"\"\"\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_reset_time > self.config.DETECTION_RESET_INTERVAL:\n",
    "            self.recent_detections.clear()\n",
    "            self.last_reset_time = current_time\n",
    "            logger.debug(\"Recent detections cleared\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main application loop\"\"\"\n",
    "        try:\n",
    "            cap = self.connect_to_stream()\n",
    "            logger.info(\"Starting vision assistance - Press 'q' to quit\")\n",
    "            \n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    logger.warning(\"Failed to grab frame, attempting reconnection\")\n",
    "                    cap.release()\n",
    "                    time.sleep(2)\n",
    "                    try:\n",
    "                        cap = self.connect_to_stream()\n",
    "                        continue\n",
    "                    except ConnectionError:\n",
    "                        logger.error(\"Failed to reconnect, exiting\")\n",
    "                        break\n",
    "                \n",
    "                # Process frame selectively\n",
    "                if self.should_process_frame():\n",
    "                    try:\n",
    "                        results = self.yolo_model(frame, verbose=False)\n",
    "                        self.process_detections(frame, results)\n",
    "                        \n",
    "                        # Display annotated frame\n",
    "                        annotated_frame = results[0].plot()\n",
    "                        cv2.imshow(\"Smart Vision Assistant\", annotated_frame)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Frame processing error: {e}\")\n",
    "                \n",
    "                # Reset detections periodically\n",
    "                self.reset_recent_detections()\n",
    "                \n",
    "                # Check for quit command\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    logger.info(\"Quit command received\")\n",
    "                    break\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Application interrupted by user\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {e}\")\n",
    "        finally:\n",
    "            self.cleanup(cap)\n",
    "    \n",
    "    def cleanup(self, cap):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        try:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            self.tts.stop()\n",
    "            logger.info(\"Cleanup completed\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cleanup error: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point\"\"\"\n",
    "    try:\n",
    "        config = Config()\n",
    "        assistant = SmartVisionAssistant(config)\n",
    "        assistant.run()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Application failed to start: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732fe757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a5cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a8671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db368e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a597049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 00:28:20,691 - INFO - Gemini API configured successfully\n",
      "2025-06-07 00:28:20,942 - INFO - YOLO model yolov8l.pt loaded successfully\n",
      "2025-06-07 00:28:20,976 - INFO - TTS engine initialized\n",
      "2025-06-07 00:28:21,011 - INFO - Connected to stream: http://192.168.1.20:81/stream\n",
      "2025-06-07 00:28:21,012 - INFO - Starting enhanced vision assistance\n",
      "2025-06-07 00:28:21,012 - INFO - Controls: 'q' to quit, 'f' to mark false positive\n",
      "2025-06-07 00:28:26,455 - INFO - Confirmed detection: person (confidence: 0.92)\n",
      "2025-06-07 00:28:26,455 - INFO - Requesting Gemini description for: person\n",
      "2025-06-07 00:28:29,838 - INFO - Gemini response: He appears to be a man with dark, somewhat unkempt hair and a beard, wearing a gray tank top; the image quality is poor, making it difficult to assess any immediate safety concerns.\n",
      "\n",
      "2025-06-07 00:28:44,321 - INFO - Confirmed detection: person (confidence: 0.75)\n",
      "2025-06-07 00:29:09,321 - INFO - Confirmed detection: person (confidence: 0.85)\n",
      "2025-06-07 00:29:09,321 - INFO - Requesting Gemini description for: person\n",
      "2025-06-07 00:29:11,400 - INFO - Gemini response: He's a muscular man with dark, curly hair and a beard, wearing a sleeveless shirt;  his posture and hand gesture suggest potential aggression, warranting caution.\n",
      "\n",
      "2025-06-07 00:29:11,400 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:29:21,655 - INFO - Confirmed detection: person (confidence: 0.85)\n",
      "2025-06-07 00:29:43,815 - INFO - Confirmed detection: person (confidence: 0.80)\n",
      "2025-06-07 00:29:43,818 - INFO - Requesting Gemini description for: person\n",
      "2025-06-07 00:29:45,779 - INFO - Gemini response: The image shows a dark-haired, possibly middle-aged man with a beard, wearing a sleeveless shirt.  He appears to be sitting indoors, and the lighting is dim, potentially posing a safety risk.\n",
      "\n",
      "2025-06-07 00:29:45,779 - ERROR - TTS error: run loop already started\n",
      "2025-06-07 00:30:01,705 - INFO - Confirmed detection: person (confidence: 0.91)\n",
      "2025-06-07 00:30:24,638 - INFO - Quit command received\n",
      "2025-06-07 00:30:24,638 - INFO - Cleanup completed\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import pyttsx3\n",
    "import io\n",
    "import threading\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Set, Optional, List\n",
    "from collections import defaultdict, deque\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration class for easy parameter management\"\"\"\n",
    "    YOLO_MODEL: str = 'yolov8l.pt'\n",
    "    SPEECH_RATE: int = 170\n",
    "    GEMINI_COOLDOWN: int = 10\n",
    "    OBJECT_COOLDOWN: int = 30\n",
    "    STREAM_URL: str = \"http://192.168.1.20:81/stream\"\n",
    "    DETECTION_RESET_INTERVAL: int = 20  # Increased from 5 to 20 seconds\n",
    "    PROCESS_EVERY_N_FRAMES: int = 2\n",
    "    MAX_GEMINI_RETRIES: int = 3\n",
    "    GEMINI_TIMEOUT: int = 10\n",
    "    \n",
    "    # New parameters for enhanced detection\n",
    "    FRAMES_FOR_CONFIRMATION: int = 2  # Object must appear in 3 consecutive frames\n",
    "    DETECTION_HISTORY_SIZE: int = 10  # Keep last 10 frames of detections\n",
    "    MIN_DETECTION_DURATION: float = 2.0  # Minimum duration in seconds\n",
    "    FALSE_POSITIVE_COOLDOWN: int = 120  # 2 minutes cooldown for false positives\n",
    "\n",
    "class DetectionTracker:\n",
    "    \"\"\"Enhanced detection tracking with temporal consistency\"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.detection_history: deque = deque(maxlen=config.DETECTION_HISTORY_SIZE)\n",
    "        self.confirmed_objects: Dict[str, float] = {}  # object -> first_detection_time\n",
    "        self.announced_objects: Set[str] = set()\n",
    "        self.false_positive_list: Set[str] = set()  # Objects marked as false positives\n",
    "        self.false_positive_timestamps: Dict[str, float] = {}\n",
    "        self.frame_detections: Dict[str, int] = defaultdict(int)  # Count consecutive frames\n",
    "        \n",
    "    def add_frame_detections(self, detections: Set[str], timestamp: float):\n",
    "        \"\"\"Add current frame detections to history\"\"\"\n",
    "        self.detection_history.append((detections, timestamp))\n",
    "        \n",
    "        # Update consecutive frame counts\n",
    "        for obj in detections:\n",
    "            self.frame_detections[obj] += 1\n",
    "        \n",
    "        # Reset counts for objects not in current frame\n",
    "        objects_to_remove = []\n",
    "        for obj in self.frame_detections:\n",
    "            if obj not in detections:\n",
    "                objects_to_remove.append(obj)\n",
    "        \n",
    "        for obj in objects_to_remove:\n",
    "            self.frame_detections[obj] = 0\n",
    "    \n",
    "    def is_detection_consistent(self, obj_name: str) -> bool:\n",
    "        \"\"\"Check if object has been detected consistently\"\"\"\n",
    "        if obj_name in self.false_positive_list:\n",
    "            # Check if cooldown period has passed\n",
    "            if obj_name in self.false_positive_timestamps:\n",
    "                if time.time() - self.false_positive_timestamps[obj_name] < self.config.FALSE_POSITIVE_COOLDOWN:\n",
    "                    return False\n",
    "                else:\n",
    "                    # Remove from false positive list after cooldown\n",
    "                    self.false_positive_list.discard(obj_name)\n",
    "                    self.false_positive_timestamps.pop(obj_name, None)\n",
    "        \n",
    "        return self.frame_detections[obj_name] >= self.config.FRAMES_FOR_CONFIRMATION\n",
    "    \n",
    "    def should_announce(self, obj_name: str, timestamp: float) -> bool:\n",
    "        \"\"\"Determine if object should be announced\"\"\"\n",
    "        # Don't announce if already announced recently\n",
    "        if obj_name in self.announced_objects:\n",
    "            return False\n",
    "        \n",
    "        # Check temporal consistency\n",
    "        if not self.is_detection_consistent(obj_name):\n",
    "            return False\n",
    "        \n",
    "        # Check minimum duration for confirmation\n",
    "        if obj_name in self.confirmed_objects:\n",
    "            duration = timestamp - self.confirmed_objects[obj_name]\n",
    "            if duration >= self.config.MIN_DETECTION_DURATION:\n",
    "                return True\n",
    "        else:\n",
    "            # First time seeing this object consistently\n",
    "            self.confirmed_objects[obj_name] = timestamp\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def mark_as_announced(self, obj_name: str):\n",
    "        \"\"\"Mark object as announced\"\"\"\n",
    "        self.announced_objects.add(obj_name)\n",
    "    \n",
    "    def mark_as_false_positive(self, obj_name: str):\n",
    "        \"\"\"Mark object as false positive (manual override)\"\"\"\n",
    "        self.false_positive_list.add(obj_name)\n",
    "        self.false_positive_timestamps[obj_name] = time.time()\n",
    "        logger.info(f\"Marked {obj_name} as false positive\")\n",
    "    \n",
    "    def reset_announced(self):\n",
    "        \"\"\"Reset announced objects (called periodically)\"\"\"\n",
    "        self.announced_objects.clear()\n",
    "        # Also clean up old confirmed objects\n",
    "        current_time = time.time()\n",
    "        old_objects = [obj for obj, timestamp in self.confirmed_objects.items() \n",
    "                      if current_time - timestamp > self.config.DETECTION_RESET_INTERVAL]\n",
    "        for obj in old_objects:\n",
    "            self.confirmed_objects.pop(obj, None)\n",
    "\n",
    "class SmartVisionAssistant:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.setup_gemini()\n",
    "        self.setup_models()\n",
    "        self.setup_tracking()\n",
    "        \n",
    "    def setup_gemini(self):\n",
    "        \"\"\"Initialize Gemini API with proper error handling\"\"\"\n",
    "        api_key = os.getenv('GOOGLE_API_KEY')\n",
    "        if not api_key:\n",
    "            logger.error(\"GOOGLE_API_KEY environment variable not set\")\n",
    "            raise ValueError(\"API key not configured\")\n",
    "        \n",
    "        try:\n",
    "            genai.configure(api_key=api_key)\n",
    "            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "            self.generation_config = genai.types.GenerationConfig(\n",
    "                temperature=0.4, \n",
    "                max_output_tokens=200\n",
    "            )\n",
    "            logger.info(\"Gemini API configured successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to configure Gemini API: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def setup_models(self):\n",
    "        \"\"\"Initialize YOLO model and TTS engine\"\"\"\n",
    "        try:\n",
    "            self.yolo_model = YOLO(self.config.YOLO_MODEL)\n",
    "            logger.info(f\"YOLO model {self.config.YOLO_MODEL} loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load YOLO model: {e}\")\n",
    "            raise\n",
    "        \n",
    "        try:\n",
    "            self.tts = pyttsx3.init()\n",
    "            self.tts.setProperty('rate', self.config.SPEECH_RATE)\n",
    "            logger.info(\"TTS engine initialized\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize TTS: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def setup_tracking(self):\n",
    "        \"\"\"Initialize tracking variables\"\"\"\n",
    "        self.detection_tracker = DetectionTracker(self.config)\n",
    "        self.gemini_last_called: Dict[str, float] = {}\n",
    "        self.global_gemini_last_called_time = 0\n",
    "        self.last_reset_time = time.time()\n",
    "        self.frame_count = 0\n",
    "        \n",
    "        # Enhanced object categories with confidence thresholds\n",
    "        self.object_confidence_thresholds = {\n",
    "            # High-priority safety objects - lower threshold\n",
    "            \"knife\": 0.60,\n",
    "            \"scissors\": 0.60,\n",
    "            \"bear\": 0.55,\n",
    "            \"fire hydrant\": 0.70,\n",
    "            \n",
    "            # Vehicles - medium threshold\n",
    "            \"truck\": 0.70,\n",
    "            \"bus\": 0.70,\n",
    "            \"car\": 0.75,\n",
    "            \"bicycle\": 0.70,\n",
    "            \"motorcycle\": 0.70,\n",
    "            \"train\": 0.65,\n",
    "            \n",
    "            # Animals - higher threshold (commonly misdetected)\n",
    "            \"dog\": 0.80,\n",
    "            \"cat\": 0.85,  # Highest threshold for cats\n",
    "            \"bird\": 0.85,\n",
    "            \n",
    "            # People and common objects\n",
    "            \"person\": 0.75,\n",
    "            \"stop sign\": 0.80,\n",
    "            \"traffic light\": 0.75,\n",
    "        }\n",
    "        \n",
    "        self.high_priority_objects = {\n",
    "            \"knife\", \"scissors\", \"fire hydrant\", \"bear\", \"truck\",\n",
    "            \"bus\", \"dog\", \"person\", \"cat\", \"car\", \"bicycle\", \n",
    "            \"motorcycle\", \"train\", \"stop sign\", \"traffic light\"\n",
    "        }\n",
    "        \n",
    "        self.local_descriptions = {\n",
    "            \"knife\": \"Warning: Sharp knife detected. Exercise caution.\",\n",
    "            \"scissors\": \"Scissors present. Handle with care.\",\n",
    "            \"person\": \"Person detected in the area.\",\n",
    "            \"dog\": \"Dog spotted nearby.\",\n",
    "            \"cat\": \"Cat detected in the vicinity.\",\n",
    "            \"car\": \"Vehicle present - stay alert.\",\n",
    "            \"truck\": \"Large truck detected.\",\n",
    "            \"bus\": \"Bus in the area.\",\n",
    "            \"bicycle\": \"Bicycle detected.\",\n",
    "            \"motorcycle\": \"Motorcycle present.\",\n",
    "            \"train\": \"Train detected - maintain safe distance.\",\n",
    "            \"fire hydrant\": \"Fire hydrant located nearby.\",\n",
    "            \"bear\": \"DANGER: Bear detected. Move to safety immediately.\",\n",
    "            \"stop sign\": \"Stop sign ahead.\",\n",
    "            \"traffic light\": \"Traffic light detected.\"\n",
    "        }\n",
    "    \n",
    "    def get_confidence_threshold(self, obj_name: str) -> float:\n",
    "        \"\"\"Get confidence threshold for specific object\"\"\"\n",
    "        return self.object_confidence_thresholds.get(obj_name, 0.70)  # Default threshold\n",
    "    \n",
    "    def speak_async(self, text: str):\n",
    "        \"\"\"Non-blocking text-to-speech\"\"\"\n",
    "        def _speak():\n",
    "            try:\n",
    "                self.tts.say(text)\n",
    "                self.tts.runAndWait()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"TTS error: {e}\")\n",
    "        \n",
    "        threading.Thread(target=_speak, daemon=True).start()\n",
    "    \n",
    "    def convert_cv2_to_bytes(self, img) -> io.BytesIO:\n",
    "        \"\"\"Convert OpenCV image to BytesIO with error handling\"\"\"\n",
    "        try:\n",
    "            _, buffer = cv2.imencode('.jpg', img, [cv2.IMWRITE_JPEG_QUALITY, 85])\n",
    "            return io.BytesIO(buffer.tobytes())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Image conversion error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def ask_gemini_about_image(self, image_bytes_io: io.BytesIO, prompt: str) -> str:\n",
    "        \"\"\"Robust Gemini API call with retries\"\"\"\n",
    "        for attempt in range(self.config.MAX_GEMINI_RETRIES):\n",
    "            try:\n",
    "                image_bytes_io.seek(0)  # Reset buffer position\n",
    "                pil_image = Image.open(image_bytes_io)\n",
    "                \n",
    "                contents = [prompt, pil_image]\n",
    "                response = self.gemini_model.generate_content(\n",
    "                    contents, \n",
    "                    generation_config=self.generation_config\n",
    "                )\n",
    "                \n",
    "                # Clean up PIL image\n",
    "                pil_image.close()\n",
    "                return response.text\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Gemini API attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == self.config.MAX_GEMINI_RETRIES - 1:\n",
    "                    return \"Unable to get detailed description at this time.\"\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        \n",
    "        return \"Description service temporarily unavailable.\"\n",
    "    \n",
    "    def connect_to_stream(self) -> cv2.VideoCapture:\n",
    "        \"\"\"Establish connection to video stream with retries\"\"\"\n",
    "        max_attempts = 3\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                cap = cv2.VideoCapture(self.config.STREAM_URL)\n",
    "                if cap.isOpened():\n",
    "                    logger.info(f\"Connected to stream: {self.config.STREAM_URL}\")\n",
    "                    return cap\n",
    "                else:\n",
    "                    logger.warning(f\"Stream connection attempt {attempt + 1} failed\")\n",
    "                    time.sleep(2)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Stream connection error: {e}\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        raise ConnectionError(f\"Failed to connect to stream after {max_attempts} attempts\")\n",
    "    \n",
    "    def process_detections(self, frame, results):\n",
    "        \"\"\"Process YOLO detection results with enhanced filtering\"\"\"\n",
    "        current_time = time.time()\n",
    "        current_frame_detections = set()\n",
    "        valid_detections = []\n",
    "        \n",
    "        # First pass: filter by confidence thresholds\n",
    "        for box in results[0].boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            conf = float(box.conf[0])\n",
    "            name = self.yolo_model.names[cls_id]\n",
    "            \n",
    "            # Apply object-specific confidence threshold\n",
    "            threshold = self.get_confidence_threshold(name)\n",
    "            if conf >= threshold:\n",
    "                current_frame_detections.add(name)\n",
    "                valid_detections.append((box, name, conf))\n",
    "                logger.debug(f\"Valid detection: {name} (confidence: {conf:.2f}, threshold: {threshold:.2f})\")\n",
    "        \n",
    "        # Update detection tracker\n",
    "        self.detection_tracker.add_frame_detections(current_frame_detections, current_time)\n",
    "        \n",
    "        # Second pass: process confirmed detections\n",
    "        for box, name, conf in valid_detections:\n",
    "            # Check if object should be announced\n",
    "            if self.detection_tracker.should_announce(name, current_time):\n",
    "                logger.info(f\"Confirmed detection: {name} (confidence: {conf:.2f})\")\n",
    "                self.speak_async(f\"{name} detected\")\n",
    "                self.detection_tracker.mark_as_announced(name)\n",
    "                \n",
    "                # Handle high-priority objects with Gemini\n",
    "                if name in self.high_priority_objects:\n",
    "                    self.handle_high_priority_object(frame, box, name, conf, current_time)\n",
    "    \n",
    "    def handle_high_priority_object(self, frame, box, name: str, conf: float, current_time: float):\n",
    "        \"\"\"Handle high-priority object detection with Gemini integration\"\"\"\n",
    "        # Check global cooldown\n",
    "        if current_time - self.global_gemini_last_called_time < self.config.GEMINI_COOLDOWN:\n",
    "            return\n",
    "        \n",
    "        # Check per-object cooldown\n",
    "        last_call = self.gemini_last_called.get(name, 0)\n",
    "        if current_time - last_call <= self.config.OBJECT_COOLDOWN:\n",
    "            return\n",
    "        \n",
    "        # Extract object region\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "        \n",
    "        if (x2 - x1) <= 0 or (y2 - y1) <= 0:\n",
    "            return\n",
    "        \n",
    "        # Process with Gemini in separate thread\n",
    "        object_crop = frame[y1:y2, x1:x2].copy()\n",
    "        threading.Thread(\n",
    "            target=self.process_with_gemini,\n",
    "            args=(object_crop, name, current_time),\n",
    "            daemon=True\n",
    "        ).start()\n",
    "    \n",
    "    def process_with_gemini(self, object_crop, name: str, timestamp: float):\n",
    "        \"\"\"Process object with Gemini API (runs in separate thread)\"\"\"\n",
    "        try:\n",
    "            img_bytes = self.convert_cv2_to_bytes(object_crop)\n",
    "            prompt = (\n",
    "                f\"Describe this {name} to a visually impaired person. \"\n",
    "                f\"Focus on safety considerations and important details. \"\n",
    "                f\"Keep it concise - 1-2 sentences maximum.\"\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Requesting Gemini description for: {name}\")\n",
    "            gemini_response = self.ask_gemini_about_image(img_bytes, prompt)\n",
    "            \n",
    "            if gemini_response and \"unavailable\" not in gemini_response.lower():\n",
    "                logger.info(f\"Gemini response: {gemini_response}\")\n",
    "                self.speak_async(gemini_response)\n",
    "                self.gemini_last_called[name] = timestamp\n",
    "                self.global_gemini_last_called_time = timestamp\n",
    "            else:\n",
    "                # Fallback to local description\n",
    "                fallback = self.local_descriptions.get(name, f\"{name} detected\")\n",
    "                self.speak_async(fallback)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gemini processing error for {name}: {e}\")\n",
    "            # Use local fallback\n",
    "            fallback = self.local_descriptions.get(name, f\"{name} detected\")\n",
    "            self.speak_async(fallback)\n",
    "    \n",
    "    def should_process_frame(self) -> bool:\n",
    "        \"\"\"Determine if current frame should be processed\"\"\"\n",
    "        self.frame_count += 1\n",
    "        return self.frame_count % self.config.PROCESS_EVERY_N_FRAMES == 0\n",
    "    \n",
    "    def reset_recent_detections(self):\n",
    "        \"\"\"Periodically reset recent detections\"\"\"\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_reset_time > self.config.DETECTION_RESET_INTERVAL:\n",
    "            self.detection_tracker.reset_announced()\n",
    "            self.last_reset_time = current_time\n",
    "            logger.debug(\"Recent detections cleared\")\n",
    "    \n",
    "    def handle_keyboard_input(self, key):\n",
    "        \"\"\"Handle keyboard commands\"\"\"\n",
    "        if key == ord('f'):  # 'f' for false positive\n",
    "            # Mark the most recently detected object as false positive\n",
    "            # This is a simple implementation - you could enhance it to show a menu\n",
    "            print(\"\\nMark object as false positive? Enter object name (or 'cancel'):\")\n",
    "            # Note: In a real implementation, you'd want to handle this more elegantly\n",
    "            # This is just a demonstration of the concept\n",
    "            pass\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main application loop\"\"\"\n",
    "        try:\n",
    "            cap = self.connect_to_stream()\n",
    "            logger.info(\"Starting enhanced vision assistance\")\n",
    "            logger.info(\"Controls: 'q' to quit, 'f' to mark false positive\")\n",
    "            \n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    logger.warning(\"Failed to grab frame, attempting reconnection\")\n",
    "                    cap.release()\n",
    "                    time.sleep(2)\n",
    "                    try:\n",
    "                        cap = self.connect_to_stream()\n",
    "                        continue\n",
    "                    except ConnectionError:\n",
    "                        logger.error(\"Failed to reconnect, exiting\")\n",
    "                        break\n",
    "                \n",
    "                # Process frame selectively\n",
    "                if self.should_process_frame():\n",
    "                    try:\n",
    "                        results = self.yolo_model(frame, verbose=False)\n",
    "                        self.process_detections(frame, results)\n",
    "                        \n",
    "                        # Display annotated frame\n",
    "                        annotated_frame = results[0].plot()\n",
    "                        cv2.imshow(\"Enhanced Smart Vision Assistant\", annotated_frame)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Frame processing error: {e}\")\n",
    "                \n",
    "                # Reset detections periodically\n",
    "                self.reset_recent_detections()\n",
    "                \n",
    "                # Check for quit command\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    logger.info(\"Quit command received\")\n",
    "                    break\n",
    "                elif key == ord('f'):\n",
    "                    self.handle_keyboard_input(key)\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Application interrupted by user\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {e}\")\n",
    "        finally:\n",
    "            self.cleanup(cap)\n",
    "    \n",
    "    def cleanup(self, cap):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        try:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            self.tts.stop()\n",
    "            logger.info(\"Cleanup completed\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cleanup error: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point\"\"\"\n",
    "    try:\n",
    "        config = Config()\n",
    "        assistant = SmartVisionAssistant(config)\n",
    "        assistant.run()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Application failed to start: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025a0662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56341ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b9679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc468559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 00:33:34,613 - INFO - Gemini API configured successfully\n",
      "2025-06-07 00:33:34,756 - INFO - YOLO model yolov8l.pt loaded successfully\n",
      "2025-06-07 00:33:34,794 - INFO - TTS engine initialized\n",
      "2025-06-07 00:33:34,856 - INFO - Connected to stream: http://192.168.1.20:81/stream\n",
      "2025-06-07 00:33:34,856 - INFO - Starting enhanced vision assistance\n",
      "2025-06-07 00:33:34,857 - INFO - Controls: 'q' to quit, 'f' to mark false positive\n",
      "2025-06-07 00:34:20,411 - INFO - Confirmed detection: person (confidence: 0.75)\n",
      "2025-06-07 00:34:20,414 - INFO - Requesting Gemini description for: person\n",
      "2025-06-07 00:34:23,656 - INFO - Gemini response for person: He appears to be a man with dark, shoulder-length hair, wearing glasses and a gray tank top;  the image quality is poor, making it difficult to assess any immediate safety concerns.\n",
      "\n",
      "2025-06-07 00:34:42,355 - INFO - Confirmed detection: person (confidence: 0.81)\n",
      "2025-06-07 00:36:07,996 - INFO - Confirmed detection: laptop (confidence: 0.91)\n",
      "2025-06-07 00:36:33,755 - INFO - Confirmed detection: person (confidence: 0.85)\n",
      "2025-06-07 00:36:33,755 - INFO - Requesting Gemini description for: person\n",
      "2025-06-07 00:36:35,754 - INFO - Gemini response for person: He's a man with dark, shoulder-length hair and a goatee, wearing a light-colored tank top; he appears agitated and is making a threatening gesture with his fist near his face, so caution is advised.\n",
      "\n",
      "2025-06-07 00:36:35,756 - ERROR - TTS error: run loop already started\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import pyttsx3\n",
    "import io\n",
    "import threading\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Set, Optional, List\n",
    "from collections import defaultdict, deque\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration class for easy parameter management\"\"\"\n",
    "    YOLO_MODEL: str = 'yolov8l.pt'\n",
    "    SPEECH_RATE: int = 170\n",
    "    GEMINI_COOLDOWN: int = 10\n",
    "    OBJECT_COOLDOWN: int = 30\n",
    "    STREAM_URL: str = \"http://192.168.1.20:81/stream\"\n",
    "    DETECTION_RESET_INTERVAL: int = 20  # Increased from 5 to 20 seconds\n",
    "    PROCESS_EVERY_N_FRAMES: int = 2\n",
    "    MAX_GEMINI_RETRIES: int = 3\n",
    "    GEMINI_TIMEOUT: int = 10\n",
    "    \n",
    "    # New parameters for enhanced detection\n",
    "    FRAMES_FOR_CONFIRMATION: int = 2  # Changed from 3 to 2 as per user request\n",
    "    DETECTION_HISTORY_SIZE: int = 10  # Keep last 10 frames of detections\n",
    "    MIN_DETECTION_DURATION: float = 2.0  # Minimum duration in seconds\n",
    "    FALSE_POSITIVE_COOLDOWN: int = 120  # 2 minutes cooldown for false positives\n",
    "\n",
    "class DetectionTracker:\n",
    "    \"\"\"Enhanced detection tracking with temporal consistency\"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.detection_history: deque = deque(maxlen=config.DETECTION_HISTORY_SIZE)\n",
    "        self.confirmed_objects: Dict[str, float] = {}  # object -> first_detection_time\n",
    "        self.announced_objects: Set[str] = set()\n",
    "        self.gemini_announced_objects: Set[str] = set()  # Separate tracking for Gemini descriptions\n",
    "        self.false_positive_list: Set[str] = set()  # Objects marked as false positives\n",
    "        self.false_positive_timestamps: Dict[str, float] = {}\n",
    "        self.frame_detections: Dict[str, int] = defaultdict(int)  # Count consecutive frames\n",
    "        \n",
    "    def add_frame_detections(self, detections: Set[str], timestamp: float):\n",
    "        \"\"\"Add current frame detections to history\"\"\"\n",
    "        self.detection_history.append((detections, timestamp))\n",
    "        \n",
    "        # Update consecutive frame counts\n",
    "        for obj in detections:\n",
    "            self.frame_detections[obj] += 1\n",
    "        \n",
    "        # Reset counts for objects not in current frame\n",
    "        objects_to_remove = []\n",
    "        for obj in self.frame_detections:\n",
    "            if obj not in detections:\n",
    "                objects_to_remove.append(obj)\n",
    "        \n",
    "        for obj in objects_to_remove:\n",
    "            self.frame_detections[obj] = 0\n",
    "    \n",
    "    def is_detection_consistent(self, obj_name: str) -> bool:\n",
    "        \"\"\"Check if object has been detected consistently\"\"\"\n",
    "        if obj_name in self.false_positive_list:\n",
    "            # Check if cooldown period has passed\n",
    "            if obj_name in self.false_positive_timestamps:\n",
    "                if time.time() - self.false_positive_timestamps[obj_name] < self.config.FALSE_POSITIVE_COOLDOWN:\n",
    "                    return False\n",
    "                else:\n",
    "                    # Remove from false positive list after cooldown\n",
    "                    self.false_positive_list.discard(obj_name)\n",
    "                    self.false_positive_timestamps.pop(obj_name, None)\n",
    "        \n",
    "        return self.frame_detections[obj_name] >= self.config.FRAMES_FOR_CONFIRMATION\n",
    "    \n",
    "    def should_announce_gemini(self, obj_name: str) -> bool:\n",
    "        \"\"\"Check if Gemini description should be announced (separate from initial detection)\"\"\"\n",
    "        return obj_name not in self.gemini_announced_objects\n",
    "    \n",
    "    def mark_gemini_as_announced(self, obj_name: str):\n",
    "        \"\"\"Mark Gemini description as announced\"\"\"\n",
    "        self.gemini_announced_objects.add(obj_name)\n",
    "    \n",
    "    def should_announce(self, obj_name: str, timestamp: float) -> bool:\n",
    "        \"\"\"Determine if object should be announced\"\"\"\n",
    "        # Don't announce if already announced recently\n",
    "        if obj_name in self.announced_objects:\n",
    "            return False\n",
    "        \n",
    "        # Check temporal consistency\n",
    "        if not self.is_detection_consistent(obj_name):\n",
    "            return False\n",
    "        \n",
    "        # Check minimum duration for confirmation\n",
    "        if obj_name in self.confirmed_objects:\n",
    "            duration = timestamp - self.confirmed_objects[obj_name]\n",
    "            if duration >= self.config.MIN_DETECTION_DURATION:\n",
    "                return True\n",
    "        else:\n",
    "            # First time seeing this object consistently\n",
    "            self.confirmed_objects[obj_name] = timestamp\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def mark_as_announced(self, obj_name: str):\n",
    "        \"\"\"Mark object as announced\"\"\"\n",
    "        self.announced_objects.add(obj_name)\n",
    "    \n",
    "    def mark_as_false_positive(self, obj_name: str):\n",
    "        \"\"\"Mark object as false positive (manual override)\"\"\"\n",
    "        self.false_positive_list.add(obj_name)\n",
    "        self.false_positive_timestamps[obj_name] = time.time()\n",
    "        logger.info(f\"Marked {obj_name} as false positive\")\n",
    "    \n",
    "    def reset_announced(self):\n",
    "        \"\"\"Reset announced objects (called periodically)\"\"\"\n",
    "        self.announced_objects.clear()\n",
    "        self.gemini_announced_objects.clear()  # Reset Gemini announcements too\n",
    "        # Also clean up old confirmed objects\n",
    "        current_time = time.time()\n",
    "        old_objects = [obj for obj, timestamp in self.confirmed_objects.items() \n",
    "                      if current_time - timestamp > self.config.DETECTION_RESET_INTERVAL]\n",
    "        for obj in old_objects:\n",
    "            self.confirmed_objects.pop(obj, None)\n",
    "\n",
    "class SmartVisionAssistant:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.setup_gemini()\n",
    "        self.setup_models()\n",
    "        self.setup_tracking()\n",
    "        \n",
    "    def setup_gemini(self):\n",
    "        \"\"\"Initialize Gemini API with proper error handling\"\"\"\n",
    "        api_key = os.getenv('GOOGLE_API_KEY')\n",
    "        if not api_key:\n",
    "            logger.error(\"GOOGLE_API_KEY environment variable not set\")\n",
    "            raise ValueError(\"API key not configured\")\n",
    "        \n",
    "        try:\n",
    "            genai.configure(api_key=api_key)\n",
    "            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "            self.generation_config = genai.types.GenerationConfig(\n",
    "                temperature=0.4, \n",
    "                max_output_tokens=200\n",
    "            )\n",
    "            logger.info(\"Gemini API configured successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to configure Gemini API: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def setup_models(self):\n",
    "        \"\"\"Initialize YOLO model and TTS engine\"\"\"\n",
    "        try:\n",
    "            self.yolo_model = YOLO(self.config.YOLO_MODEL)\n",
    "            logger.info(f\"YOLO model {self.config.YOLO_MODEL} loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load YOLO model: {e}\")\n",
    "            raise\n",
    "        \n",
    "        try:\n",
    "            self.tts = pyttsx3.init()\n",
    "            self.tts.setProperty('rate', self.config.SPEECH_RATE)\n",
    "            logger.info(\"TTS engine initialized\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize TTS: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def setup_tracking(self):\n",
    "        \"\"\"Initialize tracking variables\"\"\"\n",
    "        self.detection_tracker = DetectionTracker(self.config)\n",
    "        self.gemini_last_called: Dict[str, float] = {}\n",
    "        self.global_gemini_last_called_time = 0\n",
    "        self.last_reset_time = time.time()\n",
    "        self.frame_count = 0\n",
    "        \n",
    "        # Enhanced object categories with confidence thresholds\n",
    "        self.object_confidence_thresholds = {\n",
    "            # High-priority safety objects - lower threshold\n",
    "            \"knife\": 0.60,\n",
    "            \"scissors\": 0.60,\n",
    "            \"bear\": 0.55,\n",
    "            \"fire hydrant\": 0.70,\n",
    "            \n",
    "            # Vehicles - medium threshold\n",
    "            \"truck\": 0.70,\n",
    "            \"bus\": 0.70,\n",
    "            \"car\": 0.75,\n",
    "            \"bicycle\": 0.70,\n",
    "            \"motorcycle\": 0.70,\n",
    "            \"train\": 0.65,\n",
    "            \n",
    "            # Animals - higher threshold (commonly misdetected)\n",
    "            \"dog\": 0.80,\n",
    "            \"cat\": 0.85,  # Highest threshold for cats\n",
    "            \"bird\": 0.85,\n",
    "            \n",
    "            # People and common objects\n",
    "            \"person\": 0.75,\n",
    "            \"stop sign\": 0.80,\n",
    "            \"traffic light\": 0.75,\n",
    "        }\n",
    "        \n",
    "        self.high_priority_objects = {\n",
    "            \"knife\", \"scissors\", \"fire hydrant\", \"bear\", \"truck\",\n",
    "            \"bus\", \"dog\", \"person\", \"cat\", \"car\", \"bicycle\", \n",
    "            \"motorcycle\", \"train\", \"stop sign\", \"traffic light\"\n",
    "        }\n",
    "        \n",
    "        self.local_descriptions = {\n",
    "            \"knife\": \"Warning: Sharp knife detected. Exercise caution.\",\n",
    "            \"scissors\": \"Scissors present. Handle with care.\",\n",
    "            \"person\": \"Person detected in the area.\",\n",
    "            \"dog\": \"Dog spotted nearby.\",\n",
    "            \"cat\": \"Cat detected in the vicinity.\",\n",
    "            \"car\": \"Vehicle present - stay alert.\",\n",
    "            \"truck\": \"Large truck detected.\",\n",
    "            \"bus\": \"Bus in the area.\",\n",
    "            \"bicycle\": \"Bicycle detected.\",\n",
    "            \"motorcycle\": \"Motorcycle present.\",\n",
    "            \"train\": \"Train detected - maintain safe distance.\",\n",
    "            \"fire hydrant\": \"Fire hydrant located nearby.\",\n",
    "            \"bear\": \"DANGER: Bear detected. Move to safety immediately.\",\n",
    "            \"stop sign\": \"Stop sign ahead.\",\n",
    "            \"traffic light\": \"Traffic light detected.\"\n",
    "        }\n",
    "    \n",
    "    def get_confidence_threshold(self, obj_name: str) -> float:\n",
    "        \"\"\"Get confidence threshold for specific object\"\"\"\n",
    "        return self.object_confidence_thresholds.get(obj_name, 0.70)  # Default threshold\n",
    "    \n",
    "    def speak_async(self, text: str):\n",
    "        \"\"\"Non-blocking text-to-speech\"\"\"\n",
    "        def _speak():\n",
    "            try:\n",
    "                self.tts.say(text)\n",
    "                self.tts.runAndWait()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"TTS error: {e}\")\n",
    "        \n",
    "        threading.Thread(target=_speak, daemon=True).start()\n",
    "    \n",
    "    def convert_cv2_to_bytes(self, img) -> io.BytesIO:\n",
    "        \"\"\"Convert OpenCV image to BytesIO with error handling\"\"\"\n",
    "        try:\n",
    "            _, buffer = cv2.imencode('.jpg', img, [cv2.IMWRITE_JPEG_QUALITY, 85])\n",
    "            return io.BytesIO(buffer.tobytes())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Image conversion error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def ask_gemini_about_image(self, image_bytes_io: io.BytesIO, prompt: str) -> str:\n",
    "        \"\"\"Robust Gemini API call with retries\"\"\"\n",
    "        for attempt in range(self.config.MAX_GEMINI_RETRIES):\n",
    "            try:\n",
    "                image_bytes_io.seek(0)  # Reset buffer position\n",
    "                pil_image = Image.open(image_bytes_io)\n",
    "                \n",
    "                contents = [prompt, pil_image]\n",
    "                response = self.gemini_model.generate_content(\n",
    "                    contents, \n",
    "                    generation_config=self.generation_config\n",
    "                )\n",
    "                \n",
    "                # Clean up PIL image\n",
    "                pil_image.close()\n",
    "                return response.text\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Gemini API attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == self.config.MAX_GEMINI_RETRIES - 1:\n",
    "                    return \"Unable to get detailed description at this time.\"\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        \n",
    "        return \"Description service temporarily unavailable.\"\n",
    "    \n",
    "    def connect_to_stream(self) -> cv2.VideoCapture:\n",
    "        \"\"\"Establish connection to video stream with retries\"\"\"\n",
    "        max_attempts = 3\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                cap = cv2.VideoCapture(self.config.STREAM_URL)\n",
    "                if cap.isOpened():\n",
    "                    logger.info(f\"Connected to stream: {self.config.STREAM_URL}\")\n",
    "                    return cap\n",
    "                else:\n",
    "                    logger.warning(f\"Stream connection attempt {attempt + 1} failed\")\n",
    "                    time.sleep(2)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Stream connection error: {e}\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        raise ConnectionError(f\"Failed to connect to stream after {max_attempts} attempts\")\n",
    "    \n",
    "    def process_detections(self, frame, results):\n",
    "        \"\"\"Process YOLO detection results with enhanced filtering\"\"\"\n",
    "        current_time = time.time()\n",
    "        current_frame_detections = set()\n",
    "        valid_detections = []\n",
    "        \n",
    "        # First pass: filter by confidence thresholds\n",
    "        for box in results[0].boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            conf = float(box.conf[0])\n",
    "            name = self.yolo_model.names[cls_id]\n",
    "            \n",
    "            # Apply object-specific confidence threshold\n",
    "            threshold = self.get_confidence_threshold(name)\n",
    "            if conf >= threshold:\n",
    "                current_frame_detections.add(name)\n",
    "                valid_detections.append((box, name, conf))\n",
    "                logger.debug(f\"Valid detection: {name} (confidence: {conf:.2f}, threshold: {threshold:.2f})\")\n",
    "        \n",
    "        # Update detection tracker\n",
    "        self.detection_tracker.add_frame_detections(current_frame_detections, current_time)\n",
    "        \n",
    "        # Second pass: process confirmed detections\n",
    "        for box, name, conf in valid_detections:\n",
    "            # Check if object should be announced\n",
    "            if self.detection_tracker.should_announce(name, current_time):\n",
    "                logger.info(f\"Confirmed detection: {name} (confidence: {conf:.2f})\")\n",
    "                self.speak_async(f\"{name} detected\")\n",
    "                self.detection_tracker.mark_as_announced(name)\n",
    "                \n",
    "                # Handle high-priority objects with Gemini\n",
    "                if name in self.high_priority_objects:\n",
    "                    self.handle_high_priority_object(frame, box, name, conf, current_time)\n",
    "    \n",
    "    def handle_high_priority_object(self, frame, box, name: str, conf: float, current_time: float):\n",
    "        \"\"\"Handle high-priority object detection with Gemini integration\"\"\"\n",
    "        # Check if we should get Gemini description (separate from basic detection announcement)\n",
    "        if not self.detection_tracker.should_announce_gemini(name):\n",
    "            return\n",
    "            \n",
    "        # Check global cooldown\n",
    "        if current_time - self.global_gemini_last_called_time < self.config.GEMINI_COOLDOWN:\n",
    "            return\n",
    "        \n",
    "        # Check per-object cooldown\n",
    "        last_call = self.gemini_last_called.get(name, 0)\n",
    "        if current_time - last_call <= self.config.OBJECT_COOLDOWN:\n",
    "            return\n",
    "        \n",
    "        # Extract object region\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "        \n",
    "        if (x2 - x1) <= 0 or (y2 - y1) <= 0:\n",
    "            return\n",
    "        \n",
    "        # Mark that we're processing this object with Gemini\n",
    "        self.detection_tracker.mark_gemini_as_announced(name)\n",
    "        \n",
    "        # Process with Gemini in separate thread\n",
    "        object_crop = frame[y1:y2, x1:x2].copy()\n",
    "        threading.Thread(\n",
    "            target=self.process_with_gemini,\n",
    "            args=(object_crop, name, current_time),\n",
    "            daemon=True\n",
    "        ).start()\n",
    "    \n",
    "    def process_with_gemini(self, object_crop, name: str, timestamp: float):\n",
    "        \"\"\"Process object with Gemini API (runs in separate thread)\"\"\"\n",
    "        try:\n",
    "            img_bytes = self.convert_cv2_to_bytes(object_crop)\n",
    "            prompt = (\n",
    "                f\"Describe this {name} to a visually impaired person. \"\n",
    "                f\"Focus on safety considerations and important details. \"\n",
    "                f\"Keep it concise - 1-2 sentences maximum.\"\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Requesting Gemini description for: {name}\")\n",
    "            gemini_response = self.ask_gemini_about_image(img_bytes, prompt)\n",
    "            \n",
    "            if gemini_response and \"unavailable\" not in gemini_response.lower():\n",
    "                logger.info(f\"Gemini response for {name}: {gemini_response}\")\n",
    "                # Always speak the Gemini response - this is separate from the initial detection\n",
    "                self.speak_async(gemini_response)\n",
    "                self.gemini_last_called[name] = timestamp\n",
    "                self.global_gemini_last_called_time = timestamp\n",
    "            else:\n",
    "                # Fallback to local description\n",
    "                fallback = self.local_descriptions.get(name, f\"Detailed description of {name} unavailable\")\n",
    "                logger.info(f\"Using fallback description for {name}: {fallback}\")\n",
    "                self.speak_async(fallback)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gemini processing error for {name}: {e}\")\n",
    "            # Use local fallback\n",
    "            fallback = self.local_descriptions.get(name, f\"Unable to describe {name} in detail\")\n",
    "            self.speak_async(fallback)\n",
    "    \n",
    "    def should_process_frame(self) -> bool:\n",
    "        \"\"\"Determine if current frame should be processed\"\"\"\n",
    "        self.frame_count += 1\n",
    "        return self.frame_count % self.config.PROCESS_EVERY_N_FRAMES == 0\n",
    "    \n",
    "    def reset_recent_detections(self):\n",
    "        \"\"\"Periodically reset recent detections\"\"\"\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_reset_time > self.config.DETECTION_RESET_INTERVAL:\n",
    "            self.detection_tracker.reset_announced()\n",
    "            self.last_reset_time = current_time\n",
    "            logger.debug(\"Recent detections cleared\")\n",
    "    \n",
    "    def handle_keyboard_input(self, key):\n",
    "        \"\"\"Handle keyboard commands\"\"\"\n",
    "        if key == ord('f'):  # 'f' for false positive\n",
    "            # Mark the most recently detected object as false positive\n",
    "            # This is a simple implementation - you could enhance it to show a menu\n",
    "            print(\"\\nMark object as false positive? Enter object name (or 'cancel'):\")\n",
    "            # Note: In a real implementation, you'd want to handle this more elegantly\n",
    "            # This is just a demonstration of the concept\n",
    "            pass\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main application loop\"\"\"\n",
    "        try:\n",
    "            cap = self.connect_to_stream()\n",
    "            logger.info(\"Starting enhanced vision assistance\")\n",
    "            logger.info(\"Controls: 'q' to quit, 'f' to mark false positive\")\n",
    "            \n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    logger.warning(\"Failed to grab frame, attempting reconnection\")\n",
    "                    cap.release()\n",
    "                    time.sleep(2)\n",
    "                    try:\n",
    "                        cap = self.connect_to_stream()\n",
    "                        continue\n",
    "                    except ConnectionError:\n",
    "                        logger.error(\"Failed to reconnect, exiting\")\n",
    "                        break\n",
    "                \n",
    "                # Process frame selectively\n",
    "                if self.should_process_frame():\n",
    "                    try:\n",
    "                        results = self.yolo_model(frame, verbose=False)\n",
    "                        self.process_detections(frame, results)\n",
    "                        \n",
    "                        # Display annotated frame\n",
    "                        annotated_frame = results[0].plot()\n",
    "                        cv2.imshow(\"Enhanced Smart Vision Assistant\", annotated_frame)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Frame processing error: {e}\")\n",
    "                \n",
    "                # Reset detections periodically\n",
    "                self.reset_recent_detections()\n",
    "                \n",
    "                # Check for quit command\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    logger.info(\"Quit command received\")\n",
    "                    break\n",
    "                elif key == ord('f'):\n",
    "                    self.handle_keyboard_input(key)\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Application interrupted by user\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {e}\")\n",
    "        finally:\n",
    "            self.cleanup(cap)\n",
    "    \n",
    "    def cleanup(self, cap):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        try:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            self.tts.stop()\n",
    "            logger.info(\"Cleanup completed\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cleanup error: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point\"\"\"\n",
    "    try:\n",
    "        config = Config()\n",
    "        assistant = SmartVisionAssistant(config)\n",
    "        assistant.run()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Application failed to start: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f204de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d0629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 00:41:56,898 - INFO - Gemini API configured successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 00:41:57,038 - INFO - YOLO model yolov8l.pt loaded successfully\n",
      "2025-06-07 00:41:57,038 - ERROR - Failed to initialize TTS: 'int' object has no attribute 'DETECTION_HISTORY_SIZE'\n",
      "2025-06-07 00:41:57,038 - ERROR - Application failed to start: 'int' object has no attribute 'DETECTION_HISTORY_SIZE'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2120716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "\n",
    "class ThreadSafeTTS:\n",
    "    \"\"\"Thread-safe TTS wrapper to prevent run loop conflicts\"\"\"\n",
    "    def __init__(self, speech_rate: int = 170):\n",
    "        self.speech_queue = queue.Queue()\n",
    "        self.speech_rate = speech_rate\n",
    "        self.tts_thread = None\n",
    "        self.running = True\n",
    "        self.start_tts_worker()\n",
    "    \n",
    "    def start_tts_worker(self):\n",
    "        # ...existing code...\n",
    "        pass\n",
    "    \n",
    "    def speak(self, text: str):\n",
    "        # ...existing code...\n",
    "        pass\n",
    "    \n",
    "    def stop(self):\n",
    "        # ...existing code...\n",
    "        pass\n",
    "\n",
    "class DetectionTracker:\n",
    "    \"\"\"Enhanced detection tracking with temporal consistency\"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.detection_history: deque = deque(maxlen=config.DETECTION_HISTORY_SIZE)\n",
    "        # ...rest of DetectionTracker code...\n",
    "# ...rest of your code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86932cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 00:51:42,115 - INFO - Gemini API configured successfully\n",
      "2025-06-07 00:51:42,242 - INFO - YOLO model yolov8l.pt loaded successfully\n",
      "2025-06-07 00:51:42,242 - INFO - Thread-safe TTS engine initialized\n",
      "2025-06-07 00:51:42,276 - INFO - TTS worker thread started\n",
      "2025-06-07 00:51:42,295 - INFO - Connected to stream: http://192.168.1.20:81/stream\n",
      "2025-06-07 00:51:42,296 - INFO - Starting enhanced vision assistance\n",
      "2025-06-07 00:51:42,296 - INFO - Controls: 'q' to quit, 'f' to mark false positive\n",
      "2025-06-07 00:52:34,922 - INFO - Initiating Gemini processing for: person\n",
      "2025-06-07 00:52:34,922 - INFO - Requesting Gemini description for: person\n",
      "2025-06-07 00:52:38,356 - INFO - Gemini response for person: He appears to be a man with dark, curly hair, glasses, and a beard, wearing earphones;  be mindful of cords potentially creating a tripping hazard.\n",
      "\n",
      "2025-06-07 00:52:38,356 - INFO - Speaking: He appears to be a man with dark, curly hair, glasses, and a beard, wearing earphones;  be mindful of cords potentially creating a tripping hazard.\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import pyttsx3\n",
    "import io\n",
    "import threading\n",
    "import logging\n",
    "import queue\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Set, Optional, List\n",
    "from collections import defaultdict, deque\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration class for easy parameter management\"\"\"\n",
    "    YOLO_MODEL: str = 'yolov8l.pt'\n",
    "    SPEECH_RATE: int = 170\n",
    "    GEMINI_COOLDOWN: int = 10\n",
    "    OBJECT_COOLDOWN: int = 30\n",
    "    STREAM_URL: str = \"http://192.168.1.20:81/stream\"\n",
    "    DETECTION_RESET_INTERVAL: int = 20  # Increased from 5 to 20 seconds\n",
    "    PROCESS_EVERY_N_FRAMES: int = 2\n",
    "    MAX_GEMINI_RETRIES: int = 3\n",
    "    GEMINI_TIMEOUT: int = 10\n",
    "    \n",
    "    # New parameters for enhanced detection\n",
    "    FRAMES_FOR_CONFIRMATION: int = 2  # Changed from 3 to 2 as per user request\n",
    "    DETECTION_HISTORY_SIZE: int = 10  # Keep last 10 frames of detections\n",
    "    MIN_DETECTION_DURATION: float = 2.0  # Minimum duration in seconds\n",
    "    FALSE_POSITIVE_COOLDOWN: int = 120  # 2 minutes cooldown for false positives\n",
    "\n",
    "class ThreadSafeTTS:\n",
    "    \"\"\"Thread-safe TTS wrapper to prevent run loop conflicts\"\"\"\n",
    "    def __init__(self, speech_rate: int = 170):\n",
    "        self.speech_queue = queue.Queue()\n",
    "        self.speech_rate = speech_rate\n",
    "        self.tts_thread = None\n",
    "        self.running = True\n",
    "        self.start_tts_worker()\n",
    "    \n",
    "    def start_tts_worker(self):\n",
    "        \"\"\"Start the TTS worker thread\"\"\"\n",
    "        def tts_worker():\n",
    "            try:\n",
    "                tts_engine = pyttsx3.init()\n",
    "                tts_engine.setProperty('rate', self.speech_rate)\n",
    "                logger.info(\"TTS worker thread started\")\n",
    "                \n",
    "                while self.running:\n",
    "                    try:\n",
    "                        text = self.speech_queue.get(timeout=1)\n",
    "                        if text is None:  # Shutdown signal\n",
    "                            break\n",
    "                        logger.info(f\"Speaking: {text}\")\n",
    "                        tts_engine.say(text)\n",
    "                        tts_engine.runAndWait()\n",
    "                        self.speech_queue.task_done()\n",
    "                    except queue.Empty:\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"TTS worker error: {e}\")\n",
    "                        self.speech_queue.task_done()\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to initialize TTS worker: {e}\")\n",
    "        \n",
    "        self.tts_thread = threading.Thread(target=tts_worker, daemon=True)\n",
    "        self.tts_thread.start()\n",
    "    \n",
    "    def speak(self, text: str):\n",
    "        \"\"\"Add text to speech queue\"\"\"\n",
    "        if self.running:\n",
    "            self.speech_queue.put(text)\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the TTS worker\"\"\"\n",
    "        self.running = False\n",
    "        self.speech_queue.put(None)  # Shutdown signal\n",
    "        if self.tts_thread and self.tts_thread.is_alive():\n",
    "            self.tts_thread.join(timeout=2)\n",
    "class DetectionTracker:            \n",
    "    \"\"\"Enhanced detection tracking with temporal consistency\"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.detection_history: deque = deque(maxlen=config.DETECTION_HISTORY_SIZE)\n",
    "        self.confirmed_objects: Dict[str, float] = {}  # object -> first_detection_time\n",
    "        self.announced_objects: Set[str] = set()\n",
    "        self.gemini_announced_objects: Set[str] = set()  # Separate tracking for Gemini descriptions\n",
    "        self.false_positive_list: Set[str] = set()  # Objects marked as false positives\n",
    "        self.false_positive_timestamps: Dict[str, float] = {}\n",
    "        self.frame_detections: Dict[str, int] = defaultdict(int)  # Count consecutive frames\n",
    "        \n",
    "    def add_frame_detections(self, detections: Set[str], timestamp: float):\n",
    "        \"\"\"Add current frame detections to history\"\"\"\n",
    "        self.detection_history.append((detections, timestamp))\n",
    "        \n",
    "        # Update consecutive frame counts\n",
    "        for obj in detections:\n",
    "            self.frame_detections[obj] += 1\n",
    "        \n",
    "        # Reset counts for objects not in current frame\n",
    "        objects_to_remove = []\n",
    "        for obj in self.frame_detections:\n",
    "            if obj not in detections:\n",
    "                objects_to_remove.append(obj)\n",
    "        \n",
    "        for obj in objects_to_remove:\n",
    "            self.frame_detections[obj] = 0\n",
    "    \n",
    "    def is_detection_consistent(self, obj_name: str) -> bool:\n",
    "        \"\"\"Check if object has been detected consistently\"\"\"\n",
    "        if obj_name in self.false_positive_list:\n",
    "            # Check if cooldown period has passed\n",
    "            if obj_name in self.false_positive_timestamps:\n",
    "                if time.time() - self.false_positive_timestamps[obj_name] < self.config.FALSE_POSITIVE_COOLDOWN:\n",
    "                    return False\n",
    "                else:\n",
    "                    # Remove from false positive list after cooldown\n",
    "                    self.false_positive_list.discard(obj_name)\n",
    "                    self.false_positive_timestamps.pop(obj_name, None)\n",
    "        \n",
    "        return self.frame_detections[obj_name] >= self.config.FRAMES_FOR_CONFIRMATION\n",
    "    \n",
    "    def should_announce_gemini(self, obj_name: str) -> bool:\n",
    "        \"\"\"Check if Gemini description should be announced (separate from initial detection)\"\"\"\n",
    "        return obj_name not in self.gemini_announced_objects\n",
    "    \n",
    "    def mark_gemini_as_announced(self, obj_name: str):\n",
    "        \"\"\"Mark Gemini description as announced\"\"\"\n",
    "        self.gemini_announced_objects.add(obj_name)\n",
    "    \n",
    "    def should_announce(self, obj_name: str, timestamp: float) -> bool:\n",
    "        \"\"\"Determine if object should be announced\"\"\"\n",
    "        # Don't announce if already announced recently\n",
    "        if obj_name in self.announced_objects:\n",
    "            return False\n",
    "        \n",
    "        # Check temporal consistency\n",
    "        if not self.is_detection_consistent(obj_name):\n",
    "            return False\n",
    "        \n",
    "        # Check minimum duration for confirmation\n",
    "        if obj_name in self.confirmed_objects:\n",
    "            duration = timestamp - self.confirmed_objects[obj_name]\n",
    "            if duration >= self.config.MIN_DETECTION_DURATION:\n",
    "                return True\n",
    "        else:\n",
    "            # First time seeing this object consistently\n",
    "            self.confirmed_objects[obj_name] = timestamp\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def mark_as_announced(self, obj_name: str):\n",
    "        \"\"\"Mark object as announced\"\"\"\n",
    "        self.announced_objects.add(obj_name)\n",
    "    \n",
    "    def mark_as_false_positive(self, obj_name: str):\n",
    "        \"\"\"Mark object as false positive (manual override)\"\"\"\n",
    "        self.false_positive_list.add(obj_name)\n",
    "        self.false_positive_timestamps[obj_name] = time.time()\n",
    "        logger.info(f\"Marked {obj_name} as false positive\")\n",
    "    \n",
    "    def reset_announced(self):\n",
    "        \"\"\"Reset announced objects (called periodically)\"\"\"\n",
    "        self.announced_objects.clear()\n",
    "        self.gemini_announced_objects.clear()  # Reset Gemini announcements too\n",
    "        # Also clean up old confirmed objects\n",
    "        current_time = time.time()\n",
    "        old_objects = [obj for obj, timestamp in self.confirmed_objects.items() \n",
    "                      if current_time - timestamp > self.config.DETECTION_RESET_INTERVAL]\n",
    "        for obj in old_objects:\n",
    "            self.confirmed_objects.pop(obj, None)\n",
    "\n",
    "class SmartVisionAssistant:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.setup_gemini()\n",
    "        self.setup_models()\n",
    "        self.setup_tracking()\n",
    "        \n",
    "    def setup_gemini(self):\n",
    "        \"\"\"Initialize Gemini API with proper error handling\"\"\"\n",
    "        api_key = os.getenv('GOOGLE_API_KEY')\n",
    "        if not api_key:\n",
    "            logger.error(\"GOOGLE_API_KEY environment variable not set\")\n",
    "            raise ValueError(\"API key not configured\")\n",
    "        \n",
    "        try:\n",
    "            genai.configure(api_key=api_key)\n",
    "            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "            self.generation_config = genai.types.GenerationConfig(\n",
    "                temperature=0.4, \n",
    "                max_output_tokens=200\n",
    "            )\n",
    "            logger.info(\"Gemini API configured successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to configure Gemini API: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def setup_models(self):\n",
    "        \"\"\"Initialize YOLO model and TTS engine\"\"\"\n",
    "        try:\n",
    "            self.yolo_model = YOLO(self.config.YOLO_MODEL)\n",
    "            logger.info(f\"YOLO model {self.config.YOLO_MODEL} loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load YOLO model: {e}\")\n",
    "            raise\n",
    "        \n",
    "        try:\n",
    "            self.tts = ThreadSafeTTS(self.config.SPEECH_RATE)\n",
    "            logger.info(\"Thread-safe TTS engine initialized\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize TTS: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def setup_tracking(self):\n",
    "        \"\"\"Initialize tracking variables\"\"\"\n",
    "        self.detection_tracker = DetectionTracker(self.config)\n",
    "        self.gemini_last_called: Dict[str, float] = {}\n",
    "        self.global_gemini_last_called_time = 0\n",
    "        self.last_reset_time = time.time()\n",
    "        self.frame_count = 0\n",
    "        \n",
    "        # Enhanced object categories with confidence thresholds\n",
    "        self.object_confidence_thresholds = {\n",
    "            # High-priority safety objects - lower threshold\n",
    "            \"knife\": 0.60,\n",
    "            \"scissors\": 0.60,\n",
    "            \"bear\": 0.55,\n",
    "            \"fire hydrant\": 0.70,\n",
    "            \n",
    "            # Vehicles - medium threshold\n",
    "            \"truck\": 0.70,\n",
    "            \"bus\": 0.70,\n",
    "            \"car\": 0.75,\n",
    "            \"bicycle\": 0.70,\n",
    "            \"motorcycle\": 0.70,\n",
    "            \"train\": 0.65,\n",
    "            \n",
    "            # Animals - higher threshold (commonly misdetected)\n",
    "            \"dog\": 0.80,\n",
    "            \"cat\": 0.85,  # Highest threshold for cats\n",
    "            \"bird\": 0.85,\n",
    "            \n",
    "            # People and common objects\n",
    "            \"person\": 0.75,\n",
    "            \"stop sign\": 0.80,\n",
    "            \"traffic light\": 0.75,\n",
    "        }\n",
    "        \n",
    "        self.high_priority_objects = {\n",
    "            \"knife\", \"scissors\", \"fire hydrant\", \"bear\", \"truck\",\n",
    "            \"bus\", \"dog\", \"person\", \"cat\", \"car\", \"bicycle\", \n",
    "            \"motorcycle\", \"train\", \"stop sign\", \"traffic light\"\n",
    "        }\n",
    "        \n",
    "        self.local_descriptions = {\n",
    "            \"knife\": \"Warning: Sharp knife detected. Exercise caution.\",\n",
    "            \"scissors\": \"Scissors present. Handle with care.\",\n",
    "            \"person\": \"Person detected in the area.\",\n",
    "            \"dog\": \"Dog spotted nearby.\",\n",
    "            \"cat\": \"Cat detected in the vicinity.\",\n",
    "            \"car\": \"Vehicle present - stay alert.\",\n",
    "            \"truck\": \"Large truck detected.\",\n",
    "            \"bus\": \"Bus in the area.\",\n",
    "            \"bicycle\": \"Bicycle detected.\",\n",
    "            \"motorcycle\": \"Motorcycle present.\",\n",
    "            \"train\": \"Train detected - maintain safe distance.\",\n",
    "            \"fire hydrant\": \"Fire hydrant located nearby.\",\n",
    "            \"bear\": \"DANGER: Bear detected. Move to safety immediately.\",\n",
    "            \"stop sign\": \"Stop sign ahead.\",\n",
    "            \"traffic light\": \"Traffic light detected.\"\n",
    "        }\n",
    "    \n",
    "    def get_confidence_threshold(self, obj_name: str) -> float:\n",
    "        \"\"\"Get confidence threshold for specific object\"\"\"\n",
    "        return self.object_confidence_thresholds.get(obj_name, 0.60)  # Default threshold\n",
    "    \n",
    "    def speak_async(self, text: str):\n",
    "        \"\"\"Thread-safe text-to-speech\"\"\"\n",
    "        self.tts.speak(text)\n",
    "    \n",
    "    def convert_cv2_to_bytes(self, img) -> io.BytesIO:\n",
    "        \"\"\"Convert OpenCV image to BytesIO with error handling\"\"\"\n",
    "        try:\n",
    "            _, buffer = cv2.imencode('.jpg', img, [cv2.IMWRITE_JPEG_QUALITY, 85])\n",
    "            return io.BytesIO(buffer.tobytes())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Image conversion error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def ask_gemini_about_image(self, image_bytes_io: io.BytesIO, prompt: str) -> str:\n",
    "        \"\"\"Robust Gemini API call with retries\"\"\"\n",
    "        for attempt in range(self.config.MAX_GEMINI_RETRIES):\n",
    "            try:\n",
    "                image_bytes_io.seek(0)  # Reset buffer position\n",
    "                pil_image = Image.open(image_bytes_io)\n",
    "                \n",
    "                contents = [prompt, pil_image]\n",
    "                response = self.gemini_model.generate_content(\n",
    "                    contents, \n",
    "                    generation_config=self.generation_config\n",
    "                )\n",
    "                \n",
    "                # Clean up PIL image\n",
    "                pil_image.close()\n",
    "                return response.text\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Gemini API attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == self.config.MAX_GEMINI_RETRIES - 1:\n",
    "                    return \"Unable to get detailed description at this time.\"\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        \n",
    "        return \"Description service temporarily unavailable.\"\n",
    "    \n",
    "    def connect_to_stream(self) -> cv2.VideoCapture:\n",
    "        \"\"\"Establish connection to video stream with retries\"\"\"\n",
    "        max_attempts = 3\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                cap = cv2.VideoCapture(self.config.STREAM_URL)\n",
    "                if cap.isOpened():\n",
    "                    logger.info(f\"Connected to stream: {self.config.STREAM_URL}\")\n",
    "                    return cap\n",
    "                else:\n",
    "                    logger.warning(f\"Stream connection attempt {attempt + 1} failed\")\n",
    "                    time.sleep(2)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Stream connection error: {e}\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        raise ConnectionError(f\"Failed to connect to stream after {max_attempts} attempts\")\n",
    "    \n",
    "    def process_detections(self, frame, results):\n",
    "        \"\"\"Process YOLO detection results with enhanced filtering\"\"\"\n",
    "        current_time = time.time()\n",
    "        current_frame_detections = set()\n",
    "        valid_detections = []\n",
    "        \n",
    "        # First pass: filter by confidence thresholds\n",
    "        for box in results[0].boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            conf = float(box.conf[0])\n",
    "            name = self.yolo_model.names[cls_id]\n",
    "            \n",
    "            # Apply object-specific confidence threshold\n",
    "            threshold = self.get_confidence_threshold(name)\n",
    "            if conf >= threshold:\n",
    "                current_frame_detections.add(name)\n",
    "                valid_detections.append((box, name, conf))\n",
    "                logger.debug(f\"Valid detection: {name} (confidence: {conf:.2f}, threshold: {threshold:.2f})\")\n",
    "        \n",
    "        # Update detection tracker\n",
    "        self.detection_tracker.add_frame_detections(current_frame_detections, current_time)\n",
    "        \n",
    "        # Second pass: process confirmed detections\n",
    "        for box, name, conf in valid_detections:\n",
    "            # Check if object should be announced\n",
    "            if self.detection_tracker.should_announce(name, current_time):\n",
    "                logger.info(f\"Confirmed detection: {name} (confidence: {conf:.2f})\")\n",
    "                self.speak_async(f\"{name} detected\")\n",
    "                self.detection_tracker.mark_as_announced(name)\n",
    "            \n",
    "            # Handle high-priority objects with Gemini (separate from basic announcement)\n",
    "            if name in self.high_priority_objects:\n",
    "                self.handle_high_priority_object(frame, box, name, conf, current_time)\n",
    "    \n",
    "    def handle_high_priority_object(self, frame, box, name: str, conf: float, current_time: float):\n",
    "        \"\"\"Handle high-priority object detection with Gemini integration\"\"\"\n",
    "        # Check global cooldown\n",
    "        if current_time - self.global_gemini_last_called_time < self.config.GEMINI_COOLDOWN:\n",
    "            return\n",
    "        \n",
    "        # Check per-object cooldown\n",
    "        last_call = self.gemini_last_called.get(name, 0)\n",
    "        if current_time - last_call <= self.config.OBJECT_COOLDOWN:\n",
    "            return\n",
    "        \n",
    "        # Check if we should get Gemini description\n",
    "        if not self.detection_tracker.should_announce_gemini(name):\n",
    "            return\n",
    "        \n",
    "        # Extract object region\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "        \n",
    "        if (x2 - x1) <= 0 or (y2 - y1) <= 0:\n",
    "            return\n",
    "        \n",
    "        # Mark that we're processing this object with Gemini\n",
    "        self.detection_tracker.mark_gemini_as_announced(name)\n",
    "        logger.info(f\"Initiating Gemini processing for: {name}\")\n",
    "        \n",
    "        # Process with Gemini in separate thread\n",
    "        object_crop = frame[y1:y2, x1:x2].copy()\n",
    "        threading.Thread(\n",
    "            target=self.process_with_gemini,\n",
    "            args=(object_crop, name, current_time),\n",
    "            daemon=True\n",
    "        ).start()\n",
    "    \n",
    "    def process_with_gemini(self, object_crop, name: str, timestamp: float):\n",
    "        \"\"\"Process object with Gemini API (runs in separate thread)\"\"\"\n",
    "        try:\n",
    "            img_bytes = self.convert_cv2_to_bytes(object_crop)\n",
    "            prompt = (\n",
    "                f\"Describe this {name} to a visually impaired person. \"\n",
    "                f\"Focus on safety considerations and important details. \"\n",
    "                f\"Keep it concise - 1-2 sentences maximum.\"\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Requesting Gemini description for: {name}\")\n",
    "            gemini_response = self.ask_gemini_about_image(img_bytes, prompt)\n",
    "            \n",
    "            if gemini_response and \"unavailable\" not in gemini_response.lower():\n",
    "                logger.info(f\"Gemini response for {name}: {gemini_response}\")\n",
    "                # Always speak the Gemini response - this is separate from the initial detection\n",
    "                self.speak_async(gemini_response)\n",
    "                self.gemini_last_called[name] = timestamp\n",
    "                self.global_gemini_last_called_time = timestamp\n",
    "            else:\n",
    "                # Fallback to local description\n",
    "                fallback = self.local_descriptions.get(name, f\"Detailed description of {name} unavailable\")\n",
    "                logger.info(f\"Using fallback description for {name}: {fallback}\")\n",
    "                self.speak_async(fallback)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gemini processing error for {name}: {e}\")\n",
    "            # Use local fallback\n",
    "            fallback = self.local_descriptions.get(name, f\"Unable to describe {name} in detail\")\n",
    "            self.speak_async(fallback)\n",
    "    \n",
    "    def should_process_frame(self) -> bool:\n",
    "        \"\"\"Determine if current frame should be processed\"\"\"\n",
    "        self.frame_count += 1\n",
    "        return self.frame_count % self.config.PROCESS_EVERY_N_FRAMES == 0\n",
    "    \n",
    "    def reset_recent_detections(self):\n",
    "        \"\"\"Periodically reset recent detections\"\"\"\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_reset_time > self.config.DETECTION_RESET_INTERVAL:\n",
    "            self.detection_tracker.reset_announced()\n",
    "            self.last_reset_time = current_time\n",
    "            logger.debug(\"Recent detections cleared\")\n",
    "    \n",
    "    def handle_keyboard_input(self, key):\n",
    "        \"\"\"Handle keyboard commands\"\"\"\n",
    "        if key == ord('f'):  # 'f' for false positive\n",
    "            # Mark the most recently detected object as false positive\n",
    "            # This is a simple implementation - you could enhance it to show a menu\n",
    "            print(\"\\nMark object as false positive? Enter object name (or 'cancel'):\")\n",
    "            # Note: In a real implementation, you'd want to handle this more elegantly\n",
    "            # This is just a demonstration of the concept\n",
    "            pass\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main application loop\"\"\"\n",
    "        try:\n",
    "            cap = self.connect_to_stream()\n",
    "            logger.info(\"Starting enhanced vision assistance\")\n",
    "            logger.info(\"Controls: 'q' to quit, 'f' to mark false positive\")\n",
    "            \n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    logger.warning(\"Failed to grab frame, attempting reconnection\")\n",
    "                    cap.release()\n",
    "                    time.sleep(2)\n",
    "                    try:\n",
    "                        cap = self.connect_to_stream()\n",
    "                        continue\n",
    "                    except ConnectionError:\n",
    "                        logger.error(\"Failed to reconnect, exiting\")\n",
    "                        break\n",
    "                \n",
    "                # Process frame selectively\n",
    "                if self.should_process_frame():\n",
    "                    try:\n",
    "                        results = self.yolo_model(frame, verbose=False)\n",
    "                        self.process_detections(frame, results)\n",
    "                        \n",
    "                        # Display annotated frame\n",
    "                        annotated_frame = results[0].plot()\n",
    "                        cv2.imshow(\"Enhanced Smart Vision Assistant\", annotated_frame)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Frame processing error: {e}\")\n",
    "                \n",
    "                # Reset detections periodically\n",
    "                self.reset_recent_detections()\n",
    "                \n",
    "                # Check for quit command\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    logger.info(\"Quit command received\")\n",
    "                    break\n",
    "                elif key == ord('f'):\n",
    "                    self.handle_keyboard_input(key)\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Application interrupted by user\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {e}\")\n",
    "        finally:\n",
    "            self.cleanup(cap)\n",
    "    \n",
    "    def cleanup(self, cap):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        try:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            self.tts.stop()  # Stop the thread-safe TTS\n",
    "            logger.info(\"Cleanup completed\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cleanup error: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point\"\"\"\n",
    "    try:\n",
    "        config = Config()\n",
    "        assistant = SmartVisionAssistant(config)\n",
    "        assistant.run()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Application failed to start: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bdfa84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommend",
   "language": "python",
   "name": "recommend"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
